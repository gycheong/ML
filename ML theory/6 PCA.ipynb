{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S6$. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [Gilyoung Cheong](https://www.linkedin.com/in/gycheong/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explain how to the use of Principal Component Analysis (PCA). I thank [Steven Gubkins](https://www.linkedin.com/in/steven-gubkin-525335273/) for conversations, which helped me unwind some confusions I had regarding the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that given a real-valued random variable $x$, its **variance** is defined to be\n",
    "\n",
    "$$\\mathrm{var}(x) := \\mathbb{E}((x - \\mathbb{E}(x))^2) = \\mathbb{E}(x^2) - (\\mathbb{E}(x))^2.$$\n",
    "\n",
    "Given another real-valued random variable $y$, the **covariance** of $x$ and $y$ is defined to be\n",
    "\n",
    "$$\\mathrm{cov}(x, y) := \\mathbb{E}((x - \\mathbb{E}(x))(y - \\mathbb{E}(y))) = \\mathbb{E}(xy) - \\mathbb{E}(x)\\mathbb{E}(y)$$\n",
    "\n",
    "so that $\\mathrm{cov}(x, x) = \\mathrm{var}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $n \\geq 2$. Given $\\boldsymbol{x} := (x_1, \\dots, x_n) \\text{ and } \\boldsymbol{y} := (y_1, \\dots, y_n) \\in \\mathbb{R}^n$, the **sample covariance** of $\\boldsymbol{x}$ and $\\boldsymbol{y}$ is defined to be\n",
    "\n",
    "$$\\mathrm{scov}(\\boldsymbol{x}, \\boldsymbol{y}) := \\frac{1}{n-1}\\sum_{j=1}^n (x_j - \\bar{\\boldsymbol{x}})(y_j - \\bar{\\boldsymbol{y}}),$$\n",
    "\n",
    "where $\\bar{\\boldsymbol{x}} = (x_1 + \\cdots + x_n)/n$ and $\\bar{\\boldsymbol{y}} = (y_1 + \\cdots + y_n)/n$ are the sample means. The **sample variance** of $\\boldsymbol{x}$ is defined to be \n",
    "\n",
    "$$\\mathrm{svar}(\\boldsymbol{x}) := \\mathrm{scov}(\\boldsymbol{x}, \\boldsymbol{x}) = \\frac{1}{n-1}\\sum_{j=1}^n (x_j - \\bar{\\boldsymbol{x}})^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**. The reason for using $n-1$ instead of $n$ in the denominator for the sample variance (and the sample covariance) is that\n",
    "$$\\mathbb{E}(\\mathrm{svar}(\\boldsymbol{x})) = \\sigma^2.$$\n",
    "The use of $n-1$ in the denominator appearing in the definition of $\\mathrm{svar}(\\boldsymbol{x})$ instead of $n$ is called [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n \\in \\mathbb{R}^m$ row vectors, we write\n",
    "$$\\boldsymbol{x}_i = \\begin{bmatrix}\n",
    "x_{i1} & \\cdots & x_{in}\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "We have\n",
    "$$\\mathrm{scov}(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}) = \\frac{1}{n-1}\\sum_{k=1}^n (x_{ik} - \\bar{\\boldsymbol{x}}_i)(x_{jk} - \\bar{\\boldsymbol{x}}_j),$$\n",
    "which is the $(i,j)$-component of $XX^T/(n-1)$, where the general $(i,k)$-comonent of the $n \\times m$ matrix $X$ is $(x_{ik} - \\bar{\\boldsymbol{x}}_i)$. Hence, we say that $XX^T/(n-1)$ is the **sample covariance matrix** for the row vectors $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n \\in \\mathbb{R}^m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $S := XX^T/(n-1) = (\\mathrm{scov}(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}))_{1 \\leq i, j \\leq n}$ is an $n \\times n$ semipositive definite symmetric matrix over $\\mathbb{R}$, by the Spectral Theorem (e.g., see [this textbook, Theorem 2.1, p.166](https://www.math.brown.edu/streil/papers/LADW/LADW_2017-09-04.pdf)), there exists an $n \\times n$ orthogonal matrix $U$ such that\n",
    "$$\\frac{1}{n-1}U^{T}SU = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$$\n",
    "with $\\lambda_1 \\geq \\cdots \\geq \\lambda_n \\geq 0$. Denote by $\\boldsymbol{u}_1, \\dots, \\boldsymbol{u}_n$ the columns of $U$. We may rewrite the above as\n",
    "$$\\frac{1}{n-1}U^{T}X(U^TX)^T = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n).$$\n",
    "This makes it easier for us to see that the left-hand side of the above identity is the sample covariance matrix of the rows $\\boldsymbol{u}_1^TX, \\dots, \\boldsymbol{u}_n^TX$ of $U^TX$. More specifically, we have\n",
    "\n",
    "$$\\mathrm{scov}(\\boldsymbol{u}_i^TX, \\boldsymbol{u}_j^TX) = \\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\\lambda_i & \\text{ if } i = j \\text{ and}  \\\\\n",
    "\t0 & \\text{ if } i \\neq j.\n",
    "\t\\end{array}\\right.$$\n",
    "\n",
    "\n",
    "That is, the sample covariance of $\\boldsymbol{u}_1^TX, \\dots, \\boldsymbol{u}_n^TX$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that $\\boldsymbol{u}_1, \\dots, \\boldsymbol{u}_n$, which are columns of $U$, or equivalently rows of $U^T$, form **principal components** of $X$ (i.e., the data given by its rows $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n$) and that $\\boldsymbol{u}_i$ is an **$i$-th principal component** of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning!\n",
    "\n",
    "In the above scenario, it would be more reasonable to have $m \\geq n$, so $m$ is the number of input data entries and $n$ is the number of features. Otherewise (i.e., if $m < n$), we would automatically vanish the last $n - m$ eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use it?\n",
    "\n",
    "### Dimentionality Reduction\n",
    "\n",
    "PCA is often dubbed as a process to \"reduce dimension\", which may sound mysterious. This just means that based on our needs, we may choose the first few rows $\\boldsymbol{u}_1^TX, \\boldsymbol{u}_2^TX, \\boldsymbol{u}_3^TX, \\dots$ of $U^TX$ and ignore the rest if it it is deemed that the variance $\\lambda_i$ are quite small for $i$ closer to $n$.\n",
    "\n",
    "For example, say we are in a situation where $n > 2$ and \n",
    "$$\\frac{\\lambda_1 + \\lambda_2}{\\lambda_1 + \\lambda_2 + \\cdots + \\lambda_n} \\geq 0.98.$$\n",
    "This means that at least $98\\%$ of the total variance of $U^TX$ is concentrated on $\\boldsymbol{u}_1^TX$ and $\\boldsymbol{u}_2^TX$, so we may consider dropping every other row of $U^TX$ but these first two.\n",
    "\n",
    "Note that the columns of $\\boldsymbol{u}_i^TX$ are precisely the projections of the columns of $X$ onto $\\boldsymbol{u}_i$ because each $\\boldsymbol{u}_i$ is a unit vector, so $\\boldsymbol{u}_1^TX$ and $\\boldsymbol{u}_2^TX$ contain the projections of the columns of $X$ onto two orthogonal axes correponding to $\\boldsymbol{u}_1$ and $\\boldsymbol{u}_2$. If we look at the projections of the columns of $X$ onto any other $\\boldsymbol{u}_i$, then the data we see on the corresponding axis would have very little variance.\n",
    "\n",
    "We note that once we throw away some principal components, our data is <ins>no longer distributed with respect to the features we started with</ins> but some features (i.e., $\\boldsymbol{u}_1$ and $\\boldsymbol{u}_2$) defined as linear combinations of the given features.\n",
    "\n",
    "**Remark**. To make sure that each feature affects the total varience equally, it is a good practice to statistically normalize each feature so that its mean is $0$ and its standardard deviation is $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Regression\n",
    "\n",
    "The following is another way to use PCA in regression. It is highly influnced by [the answer to this question](https://stats.stackexchange.com/questions/24659/least-stupid-way-to-forecast-a-short-multivariate-time-series).\n",
    "\n",
    "Once we choose $U$ above, we may find in practice that the first few of $\\boldsymbol{u}_1^TX, \\boldsymbol{u}_2^TX, \\dots$ have the most of the total variation, so it is likely that we may find more obvious regression patterns in them than the later ones with less variation. After further analysis (i.e., EDA and plot visualiztions), we can extend\n",
    "* the columns of the first few rows $\\boldsymbol{u}_1^TX, \\boldsymbol{u}_2^TX, \\dots, \\boldsymbol{u}_l^TX$ by some specific regressions (e.g., linear, polynomial, logistic, etc) and the rest of\n",
    "* the columns of the rest $\\boldsymbol{u}_{l+1}^TX, \\boldsymbol{u}_{l+2}^TX, \\dots, \\boldsymbol{u}_n^TX$ by [white noise](https://en.wikipedia.org/wiki/White_noise).\n",
    "\n",
    "In the above processes, the number of columns we extend for each $\\boldsymbol{u}_{i}^TX$ is the same, and denote by $R_i$ the extened row. Then the extension of $X$ can be computed as\n",
    "$$U \\cdot \\begin{bmatrix}\n",
    "R_1 \\\\\n",
    "R_2 \\\\\n",
    "\\vdots \\\\\n",
    "R_n\n",
    "\\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
