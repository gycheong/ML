{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression (theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: [Gilyoung Cheong](https://www.linkedin.com/in/gycheong/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article explains how to create a reasonable model that involves polynomials of input data. We focus on explaining theoretical aspect of an algrorithm that uses [sklearn.preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) in conjuction with the linear regression algorithm. It is a generalized version of the algorithm introduced in [this Wikipedia article](https://en.wikipedia.org/wiki/Polynomial_regression). Although, we have not found any relevant document except coding documenations for the general case we cover, this is a well-known algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Polynomial regression\" is a [supervised machine learning](https://en.wikipedia.org/wiki/Supervised_learning) algorithm that remedies linear regression. Given input data $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m \\in \\mathbb{R}^n$ and output data $\\boldsymbol{y} = (y_1, \\dots, y_n) \\in \\mathbb{R}^n$. Unlike linear regression, we also fix $d \\in \\mathbb{Z}_{\\geq 1}$, which we use to specify maximum possible (total) degree for polynomials we use. The first case $d = 1$ is the linear regression, namely, we want to find $(\\beta_0, \\beta_1, \\dots, \\beta_m) \\in \\mathbb{R}^{m+1}$ such that\n",
    "$$\\hat{y}_i := \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}$$\n",
    "is the best approximation of $y_i$, where \n",
    "$$\\boldsymbol{x}_j = (x_{1j}, \\dots, x_{nj}) = \\begin{bmatrix}\n",
    "x_{1j} \\\\\n",
    "x_{2j} \\\\\n",
    "\\vdots \\\\\n",
    "x_{nj}\n",
    "\\end{bmatrix}.$$\n",
    "Now, the idea is that for a higher degree $d \\geq 2$, we allow $\\hat{y}_i$ to be a polynomial in $x_{i1}, \\dots, x_{im}$ with degree $\\leq d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case $d = 2$. For this case, our goal is to find $\\alpha, \\beta_1, \\dots, \\beta_m, \\gamma_1, \\dots, \\gamma_{m + 1 \\choose 2} \\in \\mathbb{R}$ such that\n",
    "$$\\hat{y}_i := \\alpha + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im} + \\gamma_1 x_{i1}^2 + \\gamma_2 x_{i1}x_{i2} + \\cdots + \\gamma_{m + 1 \\choose 2} x_{im}^2$$\n",
    "is the best approximation of $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the **polynomial feature matrix of degree $1$** for the input data $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m \\in \\mathbb{R}^n$ to be\n",
    "$$X^{(1)} = X := \\begin{bmatrix}\n",
    "\\boldsymbol{1} & \\boldsymbol{x}_1 & \\cdots & \\boldsymbol{x}_m\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1m} \\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{nm}\n",
    "\\end{bmatrix}.$$\n",
    "We recall that the linear regression $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_m) \\in \\mathbb{R}^m$ is any solution to\n",
    "$$X^{T}X\\boldsymbol{\\beta} = X^T\\boldsymbol{y},$$\n",
    "which can be also written as\n",
    "$$(X^{(1)})^TX^{(1)}\\boldsymbol{\\beta} = (X^{(1)})^T\\boldsymbol{y}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the **polynomial feature matrix of degree $2$** for the input data $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m \\in \\mathbb{R}^n$ to be\n",
    "$$X^{(2)} := \\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1m} & x_{11}^2 & x_{11}x_{12} & \\cdots & x_{1,m-1}x_{1m} & x_{1m}^2 \\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2m} & x_{21}^2 & x_{21}x_{22} & \\cdots & x_{2,m-1}x_{2m} & x_{2m}^2 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{nm} & x_{n1}^2 & x_{n1}x_{n2} & \\cdots & x_{n,m-1}x_{nm} & x_{nm}^2\n",
    "\\end{bmatrix},$$\n",
    "whose $i$-th row consists of monomials in $x_{i1}, x_{i2}, \\dots, x_{im}$ of degree $\\leq 2$ ordered by total degrees and the lexicographical order of indices. We note that $X^{(2)}$ is an $n \\times (1 + m + {m + 1 \\choose 2})$ matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **polynomial regression of degree $2$** with respect to the data $((\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m), \\boldsymbol{y})$ is $$\\boldsymbol{\\beta} = (\\beta^{(0)}, \\beta^{(1)}_1, \\dots, \\beta^{(1)}_m, \\beta^{(2)}_1, \\dots, \\beta^{(2)}_{m + 1 \\choose 2}) \\in \\mathbb{R}^{(1 + m + {m + 1 \\choose 2})}$$ such that $X^{(2)}\\boldsymbol{\\beta}$, whose $i$-th row is equal to\n",
    "$$\\beta^{(0)} + \\beta^{(1)}_1 x_{i1} + \\cdots + \\beta^{(1)}_m x_{im} + \\beta^{(2)}_1 x_{i1}^2 + \\beta^{(2)}_2 x_{i1}x_{i2} + \\cdots + \\beta^{(2)}_{m + 1 \\choose 2} x_{im}^2 \\in \\mathbb{R},$$\n",
    "is the best approximation of the given output data $\\boldsymbol{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the **polynomial feature matrix of degree $d$** for the input data $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m \\in \\mathbb{R}^n$ to be the matrix $X^{(d)}$ with $n$ rows whose $i$-th row consists of monomials in $x_{i1}, x_{i2}, \\dots, x_{im}$ of degree $\\leq d$ ordered by total degrees and the lexicographical order of indices. We note that the number of columns of $X^{(d)}$ is equal to\n",
    "$$N_{m,d} := \\sum_{j=0}^d{m - 1 + j \\choose j},$$\n",
    "each of whose summand is the number of monomials in $m$ variables of total degree $j$. As in the special cases above, the **polynomial regression of degree $d$** with respect to the data $((\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m), \\boldsymbol{y})$ is $\\boldsymbol{\\beta} \\in \\mathbb{R}^{N_{m,d}}$ such that $\\|\\boldsymbol{y} - X^{(d)}\\boldsymbol{\\beta}\\|$ is minimized. They are precisely the solutions to\n",
    "$$(X^{(d)})^TX^{(d)}\\boldsymbol{\\beta} = (X^{(d)})^T\\boldsymbol{y}.$$\n",
    "In particular, if $(X^{(d)})^TX^{(d)}$ is invertible (as an $N_{m,d} \\times N_{m,d}$ real matrix), then we get the following unique solution:\n",
    "$$\\boldsymbol{\\beta} = ((X^{(d)})^TX^{(d)})^{-1} (X^{(d)})^T\\boldsymbol{y}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**. The invertibility of $X^TX$ does not guarantee the invertibility of $(X^{(d)})^TX^{(d)}$. This can be observed with $m=1$ with any $d \\geq 2$, recalling what [the determinant of a Vandermode matrix looks like](https://en.wikipedia.org/wiki/Vandermonde_matrix)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
