{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S8.$ Linear Discriminant Analysis, Quadratic Discriminant Analysis, and Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: [Gilyoung Cheong](https://www.linkedin.com/in/gycheong/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "* [\"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman](https://hastie.su.domains/ElemStatLearn/) $\\S4.3$\n",
    "* The [Erd&#337;s Institute](https://www.erdosinstitute.org/) Data Science Boot Camp Spring 2024 Week 9 lecture notes (only available to members)\n",
    "* [Wikipedia page on conditional probability distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution)\n",
    "* [User guide page at scikit-learn](https://scikit-learn.org/stable/modules/lda_qda.html#lda-qda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General principle from Bayes rule\n",
    "\n",
    "Given any nonempty events $A, B$, we have\n",
    "$$\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B | A)\\mathbb{P}(A)}{\\mathbb{P}(B)}.$$\n",
    "\n",
    "Given any disjoint events $A_1, \\dots, A_k$, we have\n",
    "\n",
    "$$\\mathbb{P}(A_i | B) = \\frac{\\mathbb{P}(A_i)\\mathbb{P}(B | A_i)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(A_i)\\mathbb{P}(B | A_i)}{\\sum_{j=1}^k\\mathbb{P}(A_j \\cap B)} = \\frac{\\mathbb{P}(A_i)\\mathbb{P}(B | A_i)}{\\sum_{j=1}^k \\mathbb{P}(A_j) \\mathbb{P}(B | A_j)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability density functions\n",
    "\n",
    "Given a probability space $(\\Omega, \\Sigma, \\mu)$, recall that a random variable $X : \\Omega \\rightarrow \\mathbb{R}^m$ is a measurable map with respect to the Lebesgue measure on $\\mathbb{R}^m$. Given any measurable subset $A \\subset \\mathbb{R}^m$, by definition, we have\n",
    "\n",
    "$$\\mathbb{P}(X \\in A) = \\mu(\\{\\omega \\in \\Omega : X(\\omega) \\in A\\}) = \\mu(X^{-1}(A)).$$\n",
    "\n",
    "Consider the measure $\\nu$ on a sub-probability space $\\Omega$ defined by\n",
    "\n",
    "$$\\nu(X^{-1}(A)) := \\mathbb{P}(X \\in A).$$\n",
    "\n",
    "\n",
    "If $\\mu(E) = 0$ on any measurable subset $E \\subset \\Omega$ that $\\nu$ can evaluate, then $\\nu(E) = 0$, so by [Radon-Nikodym theorem](https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem), there exists a (almost) unique measurable function $f_X : \\mathbb{R}^m \\rightarrow \\mathbb{R}_{\\geq 0}$ such that\n",
    "\n",
    "$$\\mathbb{P}(X \\in A) = \\int_A f_X(\\boldsymbol{x}) d \\boldsymbol{x}.$$\n",
    "\n",
    "We call this $f_X$ the **probability density** of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional probability density functions\n",
    "\n",
    "Fix $r \\in \\mathbb{Z}_{\\geq 1}$. Given a probability space $(\\Omega, \\Sigma, \\mu)$, let \n",
    "* $X : \\Omega \\rightarrow \\mathbb{R}^m$ and\n",
    "* $Y : \\Omega \\rightarrow \\{1, 2, \\dots, r\\}$\n",
    "\n",
    "be random variables, where we use the Lebesgue measure on $\\mathbb{R}^m$ and the some probability measure on $\\{1, 2, \\dots, r\\}$ with the discrete $\\sigma$-algebra. Given any measurable subsets $A \\subset \\mathbb{R}^m$ and $B \\subset \\{1, 2, \\dots, r\\}$, we have\n",
    "\n",
    "* $\\mathbb{P}(X \\in A) = \\mu(X^{-1}(A)) = \\mu(\\{\\omega \\in \\Omega : X(\\omega) \\in A\\})$ and\n",
    "* $\\mathbb{P}(Y \\in B) = \\mu(Y^{-1}(B)) = \\mu(\\{\\omega \\in \\Omega : Y(\\omega) \\in B\\})$.\n",
    "\n",
    "Moreover, we have\n",
    "\n",
    "$$\\mathbb{P}(X \\in A, Y \\in B) = \\mu(X^{-1}(A) \\cap Y^{-1}(B)) = \\mu(\\{\\omega \\in \\Omega : X(\\omega) \\in A \\text{ and } Y(\\omega) \\in B\\}),$$\n",
    "\n",
    "so by an application of the Radon-Nikodym theorem on $(X, Y) : \\Omega \\rightarrow \\mathbb{R}^m \\times \\{1, 2, \\dots, r\\}$, we may find a (almost) unique measurable function $f_{X,Y} : \\Omega \\rightarrow \\mathbb{R}_{\\geq 0}$ such that\n",
    "\n",
    "$$\\mathbb{P}(X \\in A, Y \\in B) = \\mu(X^{-1}(A) \\cap Y^{-1}(B)) = \\int_{A \\times B} f_{X,Y}(\\boldsymbol{x}, y) d(\\boldsymbol{x} \\times y).$$\n",
    "\n",
    "Recalling that $\\{1,2, \\dots, r\\}$ has the discrete $\\sigma$-algebra, the above can be written as\n",
    "\n",
    "$$\\mathbb{P}(X \\in A, Y \\in B) = \\sum_{y \\in B} \\int_A f_{X,Y}(\\boldsymbol{x},y) d\\boldsymbol{x}.$$\n",
    "\n",
    "We call this $f_{X,Y}$ the **joint probability density function** of $X$ and $Y$.\n",
    "\n",
    "**Remark**. It follows that\n",
    "\n",
    "$$\\sum_{y=1}^k f_{X,Y}(\\boldsymbol{x}, y) = f_{X}(\\boldsymbol{x})$$\n",
    "\n",
    "for almost all $\\boldsymbol{x} \\in \\mathbb{R}^m$. From now on, we may assume that we made choices so that the above identity is true for all $\\boldsymbol{x} \\in \\mathbb{R}^m$.\n",
    "\n",
    "**Definition**. The **conditional probability density function** of $Y$ given the occurence of $X$ is defined as\n",
    "$$f_{Y|X}(\\boldsymbol{x}, y) := \\frac{f_{X,Y}(\\boldsymbol{x},y)}{f_X(\\boldsymbol{x})},$$\n",
    "\n",
    "which is only defined for almost all $\\boldsymbol{x} \\in \\mathbb{R}^m$ such that $f_X(x) > 0$. We may similarly define\n",
    "\n",
    "$$f_{X|Y}(\\boldsymbol{x}, y) := \\frac{f_{X,Y}(\\boldsymbol{x},y)}{\\mathbb{P}(Y = y)},$$\n",
    "\n",
    "for any $y \\in \\{1, 2, \\dots, r\\}$ such that $\\mathbb{P}(Y = y) > 0$.\n",
    "\n",
    "**Remark**. Note that $X$ and $Y$ are independent if and only\n",
    "$$f_{X,Y}(\\boldsymbol{x},y) = f_X(\\boldsymbol{x})f_Y(y)$$\n",
    "\n",
    "for almost all $\\boldsymbol{x} \\in \\mathbb{R}^m$ and all $y \\in \\{1, 2, \\dots, r\\}$. This is equivalent to saying that\n",
    "$$f_{Y|X}(\\boldsymbol{x}, y) = f_{Y}(y)$$\n",
    "\n",
    "for almost all $\\boldsymbol{x} \\in \\mathbb{R}^m$ such that $f_X(x) > 0$ and all $y \\in \\{1, 2, \\dots, r\\}$, which can be taken as the motivation for the defintion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation**. We write \n",
    "* $\\mathbb{P}(Y = y | X = \\boldsymbol{x}) := f_{Y|X}(\\boldsymbol{x}, y)$ and\n",
    "* $\\mathbb{P}(X = \\boldsymbol{x} | Y = y) := f_{X|Y}(\\boldsymbol{x}, y)$\n",
    "\n",
    "whenever defined. Technically, these expressions do not necessarily mean probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The following theorem is the analogoue of the principle about the Bayes rule introduced above:\n",
    "\n",
    "**Theorem**. Using the preceding notation, for any $\\boldsymbol{x} \\in \\mathbb{R}^m$ and $y \\in \\{1,2, \\dots, N\\}$, we have\n",
    "\n",
    "$$\\mathbb{P}(Y = y | X = \\boldsymbol{x}) = \\frac{\\mathbb{P}(Y = y) \\mathbb{P}(X = \\boldsymbol{x} | Y = y)}{\\sum_{j=1}^k \\mathbb{P}(y = j) \\mathbb{P}(X = \\boldsymbol{x} | Y = j)}.$$\n",
    "\n",
    "given that all the espressions are defined.\n",
    "\n",
    "*Proof*. By definition, the right-hand side is\n",
    "\n",
    "$$\\mathbb{P}(Y = y | X = \\boldsymbol{x}) = \\frac{f_{X,Y}(\\boldsymbol{x}, y)}{\\sum_{j=1}^k f_{X,Y}(\\boldsymbol{x}, j)} = \\frac{f_{X,Y}(\\boldsymbol{x}, y)}{f_{X}(\\boldsymbol{x})},$$\n",
    "\n",
    "which is identical to the left-hand side. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application in classification problems\n",
    "\n",
    "Now, the idea is to use the above theorem when we are given distinct $j, k \\in \\{1, 2, \\dots, r\\}$. Then assuming that all the expressions with the symbol $\\mathbb{P}$ is positive, having\n",
    "\n",
    "$$\\mathbb{P}(Y = j | X = \\boldsymbol{x}) = \\mathbb{P}(Y = k | X = \\boldsymbol{x})$$\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "$$\\mathbb{P}(Y = j) \\mathbb{P}(X = \\boldsymbol{x} | Y = j) = \\mathbb{P}(Y = k) \\mathbb{P}(X = \\boldsymbol{x} | Y = k).$$\n",
    "1\n",
    "When we give further assumptions on the conditional density function\n",
    "\n",
    "$$\\mathbb{P}(X = \\boldsymbol{x} | Y = y) = f_{X|Y}(\\boldsymbol{x}, y),$$\n",
    "\n",
    "the last equation can be used to cut out a hypersurface to classify data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis\n",
    "\n",
    "The following is the hypothesis we make for the Quadratic Discriminant Analysis (QDA):\n",
    "\n",
    "**Hypothesis**. Given \n",
    "* the input dataset $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n \\in \\mathbb{R}^m$ and\n",
    "* the output dataset $y_1, \\dots, y_n \\in \\{1,2, \\dots, r\\}$\n",
    "\n",
    "for training, we assume that the value for each conditional density function\n",
    "\n",
    "$$\\mathbb{P}(X = \\boldsymbol{x} | Y = k) = f_{X|Y}(\\boldsymbol{x}, k)$$\n",
    "\n",
    "is approximated as follows:\n",
    "\n",
    "$$\\mathbb{P}(X = \\boldsymbol{x} | Y = k) \\approx \\frac{1}{(2\\pi)^{m/2} (\\det(\\Sigma_k))^{1/2}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu_k})^T \\Sigma_k^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu_k})\\right),$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\boldsymbol{\\mu_k} := \\frac{1}{N_k}\\sum_{\\substack{1\\leq i \\leq n \\\\ y_i = k}} \\boldsymbol{x}_i$,\n",
    "* $N_k := \\#\\{i \\in \\{1, 2, \\dots, n\\} : y_i = k\\}$, and\n",
    "* $\\Sigma_k = \\frac{1}{N_k - 1}\\sum_{\\substack{1\\leq i \\leq n \\\\ y_i = k}}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_i)(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_i)^T$, the $m \\times m$ matrix each of whose entry is the corresponding sample covariance conditional on $Y = k$.\n",
    "\n",
    "We also have the approximation\n",
    "\n",
    "$$\\mathbb{P}(Y = j) \\approx N_k / n,$$\n",
    "\n",
    "the uniform probability that a random $y_i$ among our training data is equal to $k$.\n",
    "\n",
    "**Remark**. The above approximation is an approximate version of a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). When $\\Sigma_k$ is not invertible, we may use its [pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse), which can be constructed by taking the transpose of the singular decomposition and then replacing the nonzero singular values with their reciprocals. The determinant can be replaced by the product of nonzero eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the classification hypersurfaces may be approximately given by\n",
    "\n",
    "$$N_j \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu_j})^T \\Sigma_j^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu_j})\\right) = N_k \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu_k})^T \\Sigma_k^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu_k})\\right)$$\n",
    "\n",
    "for distinct $j, k \\in \\{1, 2, \\dots, r\\}$. Assuming each $N_j \\neq 0$, we may take the logarithm to have\n",
    "\n",
    "$$\\log(N_j) - \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu_j})^T \\Sigma_j^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu_j}) = \\log(N_k) - \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu_k})^T \\Sigma_k^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu_k}),$$\n",
    "\n",
    "each of which is a quadratic equation in $\\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "\n",
    "The following is the hypothesis we make for the Linear Discriminant Analysis (LDA):\n",
    "\n",
    "**Hypothesis**. Given \n",
    "* the input dataset $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n \\in \\mathbb{R}^m$ and\n",
    "* the output dataset $y_1, \\dots, y_n \\in \\{1,2, \\dots, r\\}$\n",
    "\n",
    "for training, we assume that the value for each conditional density function\n",
    "\n",
    "$$\\mathbb{P}(X = \\boldsymbol{x} | Y = k) = f_{X|Y}(\\boldsymbol{x}, k)$$\n",
    "\n",
    "is approximated as follows:\n",
    "\n",
    "$$\\mathbb{P}(X = \\boldsymbol{x} | Y = k) \\approx \\frac{1}{(2\\pi)^{m/2} (\\det(\\Sigma))^{1/2}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu_k})^T \\Sigma^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu_k})\\right),$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\boldsymbol{\\mu_k} := \\frac{1}{N_k}\\sum_{\\substack{1\\leq i \\leq n \\\\ y_i = k}} \\boldsymbol{x}_i$,\n",
    "* $N_k := \\#\\{i \\in \\{1, 2, \\dots, n\\} : y_i = k\\}$, and\n",
    "* $\\Sigma = \\frac{1}{n- 1}\\sum_{k=1}^r\\sum_{\\substack{1\\leq i \\leq n \\\\ y_i = k}}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_i)(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_i)^T$,  the $m \\times m$ matrix each of whose entry is the corresponding sample covariance.\n",
    "\n",
    "For non-invertible $\\Sigma$, we may apply the same strategy as in QDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming each $N_j \\neq 0$, for distinct $j, k \\in \\{1, 2, \\dots, r\\}$, the classification hypersurface is given by\n",
    "\n",
    "$$\\log(N_j) - \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu_j})^T \\Sigma^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu_j}) = \\log(N_k) - \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu_k})^T \\Sigma^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu_k}).$$\n",
    "\n",
    "Since $\\Sigma$ is symmetric, so is $\\Sigma^{-1}$, and we may cancel out the quadratic terms in the above equation to rewrite it as\n",
    "\n",
    "$$\\log(N_j) + \\boldsymbol{x}\\Sigma^{-1}\\boldsymbol{\\mu}_j - \\frac{1}{2}\\boldsymbol{\\mu_j}^T \\Sigma^{-1} \\boldsymbol{\\mu_j} = \\log(N_k) + \\boldsymbol{x}\\Sigma^{-1}\\boldsymbol{\\mu}_k - \\frac{1}{2}\\boldsymbol{\\mu_k}^T \\Sigma^{-1} \\boldsymbol{\\mu_k},$$\n",
    "\n",
    "which is linear in $\\boldsymbol{x}$. Hence in this case, we get classifying hyperplanes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "We start with the theorem above:\n",
    "\n",
    "$$\\mathbb{P}(Y = y | X = \\boldsymbol{x}) = \\frac{\\mathbb{P}(Y = y) \\mathbb{P}(X = \\boldsymbol{x} | Y = y)}{\\sum_{j=1}^k \\mathbb{P}(y = j) \\mathbb{P}(X = \\boldsymbol{x} | Y = j)}.$$\n",
    "\n",
    "Then we also assume that the random variable $X = (X_1, X_2, \\dots, X_m) \\in \\mathbb{R}^m$ is given independently by random variables $X_1, \\dots, X_m \\in \\mathbb{R}$. That is, writing $\\boldsymbol{x} = (u_1, \\dots, u_m)$, for any $k \\in \\{1, 2, \\dots, r\\}$, we assume that\n",
    "\n",
    "$$\\mathbb{P}(X = \\boldsymbol{x} | Y = y) = \\mathbb{P}(X_1 = u_1 | Y = k) \\cdots \\mathbb{P}(X_m = u_m | Y = k).$$\n",
    "\n",
    "Then we assume that\n",
    "\n",
    "$$\\mathbb{P}(X_j = u_j | Y = k) \\approx \\frac{1}{(2\\pi \\sigma_{k}^2)^{1/2}}\\exp\\left(-\\frac{1}{2\\sigma_{k}^2}(u_j - \\mu_{k})^2\\right),$$\n",
    "\n",
    "where writing $\\boldsymbol{x}_i = (x_{1i}, \\dots, x_{ni})$, we have\n",
    "\n",
    "* $\\mu_{k} := \\frac{1}{N_k}\\sum_{\\substack{1\\leq i \\leq n \\\\ y_i = k}} x_{ki}$,\n",
    "* $N_k := \\#\\{i \\in \\{1, 2, \\dots, n\\} : y_i = k\\}$, and\n",
    "* $\\sigma_k^2 = \\frac{1}{N_k - 1}\\sum_{\\substack{1\\leq i \\leq n \\\\ y_i = k}}(x_{ki} - \\mu_i)^2$, the sample covariance under $Y = k$.\n",
    "\n",
    "Then we get classifying quadratic hypersurfaces. (If we replace $\\sigma_k$ with some constant quauntity, then we get classifying hyperplanes). Since we approximiate each $\\mathbb{P}(X_j = u_j | Y = k)$ by Gaussian distribution (a.k.a. normal distribution), this classification model is called the **Gaussian Naive Bayes** Classifier. Using different dirstributions yield different types of classification models, which can be found in this [scikit-learn user guide page](https://scikit-learn.org/stable/modules/naive_bayes.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
