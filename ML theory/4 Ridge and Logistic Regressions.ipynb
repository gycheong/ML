{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S4.$ Ridge & Logistic Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: [Gilyoung Cheong](https://www.linkedin.com/in/gycheong/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "* [\"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman](https://hastie.su.domains/ElemStatLearn/) -- Chapter 3\n",
    "* [Lecture notes on ridge rigression by Wessel N. van Wieringen](https://arxiv.org/abs/1509.09169)\n",
    "* [Wikipedia page about ridge regresion](https://en.wikipedia.org/wiki/Ridge_regression)\n",
    "* [A Stack Exchange discussion about ridge regression](https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution) -- Answer by whuber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement**\n",
    "\n",
    "I thank [Steven Gubkin](https://www.linkedin.com/in/steven-gubkin-525335273/) and [Paul VanKoughnett](https://www.linkedin.com/in/paul-vankoughnett/) for personal communcations through the [Erd≈ës Institute](https://www.erdosinstitute.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "\"Ridge regression\" is a model that takes a possibility that <u>there can be a relation among covariances of input data</u> into a consideration when we use linear or polynomial regressions. (This point is explained in more detail in the notebook.)\n",
    "\n",
    "Given input data $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m \\in \\mathbb{R}^m$ and output data $\\boldsymbol{y} = (y_1, \\dots, y_m) \\in \\mathbb{R}^n$. The goal of [linear regression](https://github.com/gycheong/machine_learning/blob/main/ML%20theory/1%20Linear%20Regression.ipynb) is to find $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_m) \\in \\mathbb{R}^{m+1}$ such that with\n",
    "$$X = \\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1m} \\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{nm}\n",
    "\\end{bmatrix}$$\n",
    "and $$\\boldsymbol{x}_j = (x_{1j}, \\dots, x_{nj}) = \\begin{bmatrix}\n",
    "x_{1j} \\\\ \\vdots \\\\ x_{nj}\n",
    "\\end{bmatrix}$$\n",
    "for $1 \\leq j \\leq m$, the $n$-vector $X\\boldsymbol{\\beta}$ is the best possible approximation of $\\boldsymbol{y}$ in the sense that $\\|\\boldsymbol{y} - X\\boldsymbol{\\beta}\\|$ is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Now, the idea is that when we have large input data, different features become highly correlated. One suggested remedy is to penalize the length of the linear coefficients, or equivalently its square: $\\beta_1^2 + \\cdots + \\beta_m^2$. That is, given $t \\in (0, \\infty]$, we want to\n",
    "\n",
    "* minimize $f_0(\\beta_0, \\beta_1, \\dots, \\beta_m) = \\|\\boldsymbol{y} - X\\boldsymbol{\\beta}\\|^2 = \\sum_{i=1}^n (y_j - (\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}))^2$ subject to\n",
    "* $f_1(\\beta_0, \\beta_1, \\dots, \\beta_m) = \\| \\boldsymbol{\\beta} \\|^2 = \\beta_1^2 + \\cdots + \\beta_m^2 \\leq c$.\n",
    "\n",
    "Note that $c = \\infty$ is identical to the linear regression, so we may assume that $c > 0$ is finite. Since both $f_0$ and $f_1$ are convex functions defined on $\\mathbb{R}^{m+1}$, we may check the [Slater's condition](https://github.com/gycheong/machine_learning/blob/main/ML%20theory/3%20Lagrange%20duality%20and%20Slater's%20condition.ipynb) to see if we can get obtain the minimum using the Langrange duality of the problem, and indeed, the zero vector is a strictly feasible input in the (relative) interior of the domain. The Lagrangian is given as a function $L : \\mathbb{R}^{m+1} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ defined by\n",
    "\n",
    "$$\\begin{align*} L(\\boldsymbol{\\beta}, \\lambda) &= f_{0}(\\boldsymbol{\\beta}) + \\lambda f_1(\\boldsymbol{\\beta}) \\\\\n",
    "&= \\|\\boldsymbol{y} - X\\boldsymbol{\\beta}\\|^2 + \\lambda(\\| \\boldsymbol{\\beta} \\|^2 - c) \\\\\n",
    "&= (\\boldsymbol{y} - X\\boldsymbol{\\beta})^T(\\boldsymbol{y} - X\\boldsymbol{\\beta}) + \\lambda (\\boldsymbol{\\beta}^T\\boldsymbol{\\beta} - c)\n",
    "\\end{align*}$$\n",
    "\n",
    "and thus the Lagrangian dual function is given as a function $g : \\mathbb{R} \\rightarrow \\mathbb{R}$ defined by\n",
    "\n",
    "$$g(\\lambda) = \\inf_{\\boldsymbol{\\beta}\\in\\mathbb{R}^{m+1}}(L(\\boldsymbol{\\beta}, \\lambda)),$$\n",
    "\n",
    "and maximizing $g(\\lambda)$ subject to $\\lambda \\geq 0$ necessarily solves the optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computaton of the Lagrange dual function\n",
    "\n",
    "Given $\\lambda \\geq 0$, we compute $g(\\lambda)$ under a mild condition. Consider the $(m+1) \\times (n + m + 1)$ matrix\n",
    "$$X_{\\sqrt{\\lambda}} := \\begin{bmatrix}\n",
    "X \\\\\n",
    "\\sqrt{\\lambda} I_{m+1}\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "where $I_{m+1}$ is the $(m+1) \\times (m+1)$ identity matrix. One can check that $X_{\\sqrt{\\lambda}}^{T}X_{\\sqrt{\\lambda}} = X^TX + \\lambda I_{m+1}$. Considering the $(n+m+1) \\times 1$ column matrix\n",
    "$$\\boldsymbol{y}' := \\begin{bmatrix}\n",
    "\\boldsymbol{y} \\\\\n",
    "\\boldsymbol{0}\n",
    "\\end{bmatrix},$$\n",
    "and note that ${\\boldsymbol{y}'}^T\\boldsymbol{y}' = \\boldsymbol{y}^T\\boldsymbol{y}$ and $X_{\\sqrt{\\lambda}}^T\\boldsymbol{y}' = X^T\\boldsymbol{y}$. Hence, it follows that\n",
    "$$\\begin{align*}\n",
    "\\|\\boldsymbol{y}' - X_{\\sqrt{\\lambda}}\\boldsymbol{\\beta}\\|^2 &= (\\boldsymbol{y}' - X_{\\sqrt{\\lambda}}\\boldsymbol{\\beta})^T(\\boldsymbol{y}' - X_{\\sqrt{\\lambda}}\\boldsymbol{\\beta}) \\\\\n",
    "&= ({\\boldsymbol{y}'}^T - \\boldsymbol{\\beta}^TX_{\\sqrt{\\lambda}}^T)(\\boldsymbol{y}' - X_{\\sqrt{\\lambda}}\\boldsymbol{\\beta}) \\\\\n",
    "&= \\boldsymbol{y}^T\\boldsymbol{y} - \\boldsymbol{\\beta}^TX\\boldsymbol{y} - \\boldsymbol{y}X\\boldsymbol{\\beta} + \\boldsymbol{\\beta}^T(X^TX + \\lambda I_{m+1})\\boldsymbol{\\beta} \\\\\n",
    "&= (\\boldsymbol{y} - X\\boldsymbol{\\beta})^T(\\boldsymbol{y} - X\\boldsymbol{\\beta}) + \\lambda \\boldsymbol{\\beta}^T\\boldsymbol{\\beta} \\\\\n",
    "&= \\|\\boldsymbol{y} - X\\boldsymbol{\\beta}\\|^2 + \\lambda \\|\\boldsymbol{\\beta}\\|^2\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall from a [previous discussion about the linear regression](https://github.com/gycheong/machine_learning/blob/main/Linear%20and%20Polynomial%20Regressions/Linear%20Regression%20(theory).ipynb) that the set of all $\\boldsymbol{\\beta}$ that minimizes the above quantity are precisely the ones that satisfy\n",
    "\n",
    "$$X_{\\sqrt{\\lambda}}^{T}X_{\\sqrt{\\lambda}} \\boldsymbol{\\beta} = X_{\\sqrt{\\lambda}}^{T}\\boldsymbol{y}',$$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$(X^{T}X + \\lambda I_{m+1}) \\boldsymbol{\\beta} = X^{T}\\boldsymbol{y}.$$\n",
    "\n",
    "This shows the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma** (Explicit Lagrange dual function). For any $\\lambda \\geq 0$ such that $X^{T}X + \\lambda I_{m+1}$ is invertible, we have\n",
    "$$g(\\lambda) = \\|\\boldsymbol{y} - X\\boldsymbol{\\beta}_{\\lambda}\\|^2 + \\lambda (\\|\\boldsymbol{\\beta}_{\\lambda}\\|^2 - c)$$\n",
    "\n",
    "with $\\boldsymbol{\\beta}_{\\lambda} = (X^{T}X + \\lambda I_{m+1})^{-1}X^{T}\\boldsymbol{y}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invertibility of Covariance Matrix as Motivation behind Ridge Regression\n",
    "\n",
    "Note that $\\lambda = 0$ corresponds to the usual linear regression, which corresponds to $t = \\infty$ in the optimization problem above. If we normalize our data to assume that each column has mean $0$, then using a similar discussion to the [beginning of our discussion about PCA](https://github.com/gycheong/machine_learning/blob/main/ML%20theory/6%20PCA.ipynb) says that entries of $m^{-1}X^TX$ are given by sample covariances of the input data. Thus, if $X^TX$ is NOT invertible, then it means that the these covariances has a relation, namely $\\det(X^TX) = 0$.\n",
    "\n",
    "The effect of adding penality $\\lambda$ can make us avoid singular matrix because\n",
    "\n",
    "$$\\det(X^TX + \\lambda I_{m+1}) = (-1)^{m+1} P_{X^TX}(-\\lambda),$$\n",
    "\n",
    "where $P_{X^TX}(t)$ is the characteritic polynomial of $X^TX$. Since $X^TX$ is a real symmetric matrix, the roots of $P_{X^TX}(t)$ are necessarily nonnegative real numbers. Hence, for any $\\lambda > 0$, the matrix $X^TX + \\lambda I_{m+1}$ is nonsingular. (This was kindly noted by Steven Gubkin.) This is a major reason for using Ridge Regression!\n",
    "\n",
    "**Remark**. Note that the polynomial regression is just a linear regression of the polynomial features of the given input data, so we can make sense of the ridge polynomial regression as well.\n",
    "\n",
    "### Relationship between $c$ and $\\lambda^*$ \n",
    "\n",
    "From now on, let us assume that $n \\geq m + 1$, which is a fair assumption in practice when $n$ is the number of indices and $m$ is the number of features in a given input dataframe.\n",
    "\n",
    "Since we checked that our optimization problem satsifies [Slater's condition](https://github.com/gycheong/machine_learning/blob/main/ML%20theory/3%20Lagrange%20duality%20and%20Slater's%20condition.ipynb), as long as our original optimization problem has a solution $\\boldsymbol{\\beta}^* \\in \\mathbb{R}^{m+1}$, there must be some $\\lambda^* \\in \\mathbb{R}_{\\geq 0}$ such that $g(\\lambda^*)$ is equal to the optimum. It must follow that\n",
    "$$\\|\\boldsymbol{y} - X\\boldsymbol{\\beta}_{\\lambda^*}\\|^2 + \\lambda^* (\\|\\boldsymbol{\\beta}_{\\lambda^*}\\|^2 - c)  = g(\\lambda^*) = f_0(\\beta^*) = \\|\\boldsymbol{y} - X\\boldsymbol{\\beta}^*\\|^2$$\n",
    "and if $\\lambda^*=0$, then our situation is the same as the usual linear regression, namely $c = \\infty$.\n",
    "\n",
    "Hence, we focus on the case $\\lambda^* > 0$. Then\n",
    "\n",
    "$$c = \\|\\boldsymbol{\\beta}_{\\lambda^*}\\|^2 + \\frac{\\|\\boldsymbol{y} - X\\boldsymbol{\\beta}_{\\lambda^*}\\|^2 - \\|\\boldsymbol{y} - X\\boldsymbol{\\beta}^*\\|^2}{\\lambda^*},$$\n",
    "\n",
    "so we can understand $c$ as a function in $\\lambda^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the relationship between $c$ and $\\lambda^*$ better, we use an [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) of $X = U D V^T$, where\n",
    "* $U$ is an $n \\times n$ orthogonal matrix,\n",
    "* $D$ is an $n \\times (m+1)$ matrix such that all but possibly diagonal entries $\\alpha_0, \\dots, \\alpha_{m}$ are $0$, and\n",
    "* $V$ is an $(m+1) \\times (m+1)$ orthogonal matrix.\n",
    "\n",
    "This implies that $X^T = V D^T U^T$ and thus $X^TX = V\\mathrm{diag}(\\alpha_0^2, \\dots, \\alpha_{m}^2)V^T,$ which tells us that\n",
    "$$X^TX + \\lambda I_{m+1} = V\\mathrm{diag}(\\alpha_0^2 + \\lambda, \\dots, \\alpha_m^2 + \\lambda)V^T,$$\n",
    "\n",
    "so for any $\\lambda > 0$, we have\n",
    "\n",
    "$$(X^TX + \\lambda I_{m+1})^{-1} = V\\mathrm{diag}((\\alpha_0^2 + \\lambda)^{-1}, \\dots, (\\alpha_m^2 + \\lambda)^{-1})V^T.$$\n",
    "\n",
    "This implies that\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{\\beta}_{\\lambda} &= (X^{T}X + \\lambda I_{m+1})^{-1}X^{T}\\boldsymbol{y} \\\\\n",
    "&= V\\mathrm{diag}((\\alpha_0^2 + \\lambda)^{-1}, \\dots, (\\alpha_m^2 + \\lambda)^{-1})D^TU^T\\boldsymbol{y} \\\\\n",
    "&= V A_{\\lambda} U^T\\boldsymbol{y},\n",
    "\\end{align*}$$\n",
    "\n",
    "where $A_{\\lambda}$ is an $(m+1) \\times n$ matrix whose \n",
    "* $i$-th diagonal entry is $\\alpha_i(\\alpha_i^2 + \\lambda)^{-1}$ and\n",
    "* all of whose non-diagonal entries are $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that\n",
    "\n",
    "$$\\begin{align*}\n",
    "X\\boldsymbol{\\beta}_{\\lambda} &= U DA_{\\lambda} U^T\\boldsymbol{y} \\\\\n",
    "&= U \\mathrm{diag}(\\alpha_0(\\alpha_0^2 + \\lambda)^{-1}, \\dots, \\alpha_m(\\alpha_m^2 + \\lambda)^{-1}, 0, 0, \\dots, 0) U^T\\boldsymbol{y}\n",
    "\\end{align*}$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\\boldsymbol{y} - X\\boldsymbol{\\beta}_{\\lambda\n",
    "\n",
    "} = U\\mathrm{diag}(1 - \\alpha_0(\\alpha_0^2 + \\lambda)^{-1}, \\dots, 1 - \\alpha_m(\\alpha_m^2 + \\lambda)^{-1}, 1, 1 , \\dots, 1) U^T\\boldsymbol{y}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $U$ and $V$ are orthogonal, it follows that\n",
    "\n",
    "$$\\|\\boldsymbol{y} - X\\boldsymbol{\\beta}_{\\lambda}\\|^2 = \\sum_{i=0}^{m}\\left(1 - \\frac{\\alpha_i}{\\alpha_i^2 + \\lambda}\\right)^2z_i^2 + \\sum_{j=m+1}^nz_j^2$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\|\\boldsymbol{\\beta}_{\\lambda}\\|^2 &= \\|V A_{\\lambda} U^T\\boldsymbol{y}\\|^2 \\\\\n",
    "&= \\|A_{\\lambda} U^T\\boldsymbol{y}\\|^2 \\\\\n",
    "&= \\sum_{i=0}^m\\frac{\\alpha_i^2z_i^2}{(\\alpha_i^2 + \\lambda)^{2}},\n",
    "\\end{align*}$$\n",
    "\n",
    "writing $U^T\\boldsymbol{y} = (z_0, z_1, \\dots, z_m, z_{m+1}, \\dots, z_n)$. This shows the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition**. Keeping the above notation, we have\n",
    "\n",
    "$$\\|\\boldsymbol{y} - X\\boldsymbol{\\beta}_{\\lambda}\\|^2 + \\lambda \\|\\boldsymbol{\\beta}_{\\lambda}\\|^2 = \\sum_{j=m+1}^nz_j^2 + \\sum_{i=0}^m\\left[\\left(1 - \\frac{\\alpha_i}{\\alpha_i^2 + \\lambda}\\right)^2z_i^2 + \\frac{\\lambda \\alpha_i^2z_i^2}{(\\alpha_i^2 + \\lambda)^{2}}\\right],$$\n",
    "\n",
    "where $U^T\\boldsymbol{y} = (z_0, z_1, \\dots, z_m, z_{m+1}, \\dots, z_n)$.\n",
    "\n",
    "**Remark** We can observe that the summand on the right-hand side is eventually decreasing in $\\lambda$ as long as $\\alpha_i > 0$. We already know that $\\alpha_i \\geq 0$, and unless $X = 0$, we know at least one $\\alpha_i$ is positive. For $\\lambda^* > 0$ large enough (the interval of which can probably be made more quantitative), we see that $c$ is uniquely determined by\n",
    "\n",
    "$$\\begin{align*}\n",
    "c &= \\|\\boldsymbol{\\beta}_{\\lambda^*}\\|^2 + \\frac{\\|\\boldsymbol{y} - X\\boldsymbol{\\beta}_{\\lambda^*}\\|^2 - \\|\\boldsymbol{y} - X\\boldsymbol{\\beta}^*\\|^2}{\\lambda^*} \\\\\n",
    "&= \\sum_{i=0}^{m}\\left[\\frac{\\alpha_i^2z_i^2}{(\\alpha_i^2 + \\lambda^*)^{2}} + \\left(1 - \\frac{\\alpha_i}{\\alpha_i^2 + \\lambda^*}\\right)^2\\frac{z_i^2}{\\lambda^*}\\right] + \\sum_{j=m+1}^n\\frac{z_j^2}{\\lambda^*} - \\frac{\\|\\boldsymbol{y} - X\\boldsymbol{\\beta}^*\\|^2}{\\lambda^*}\n",
    "\\end{align*}$$\n",
    "\n",
    "and $c$ is a decreasing function in $\\lambda^*$. With such uniqueness $\\lambda^* > 0$ is determined by a choice of $c$, and we note that the large values of $\\lambda^*$ generally mean smaller $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Generally speaking a \"logistic regression\" is applying any regression techniques for classifying purposes. The idea is very simple: if we have a model $\\phi : \\mathbb{R}^n \\times \\mathbb{R}^{m} \\rightarrow \\mathbb{R}$, feeding this model into the **sigmoid function** $\\sigma : \\mathbb{R} \\rightarrow [0, 1]$ defined by\n",
    "$$\\sigma(t) := \\frac{1}{1 + e^{-t}}$$\n",
    "\n",
    "would give us the model $\\sigma \\circ \\phi : \\mathbb{R}^n \\times \\mathbb{R}^{m} \\rightarrow [0,1]$ that could be trained for predicting the probability distribution of a categorical outcome.\n",
    "\n",
    "**Remark**. Although one can feed any regression model into the sigmoid function to create a logistic version of such regression model, but it seems that when we discuss a logistic regression, it is assumed that the regression is a logistic linear ridge regression, as we can see in the documentation of the [LogisticRegression from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Since any polynomial regression is a linear regression on the polynomial features of the input data, such a library can be directly used when we consider a logistic polynomial (ridge) regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
