{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: [Gilyoung Cheong](https://www.linkedin.com/in/gycheong/)\n",
    "\n",
    "**References**\n",
    "* [\"The Elements of Statistical Learning\" by Hastie, Tibshirani, Friedman](https://hastie.su.domains/ElemStatLearn/): Chapter 12\n",
    "* [Wikipedia page about radial basis function kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)\n",
    "* [Wikipedia page about support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "* [\"Convex Optimization\" by Boyd and Vandenberghe](https://web.stanford.edu/~boyd/cvxbook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machine is a [supervised machine learning](https://en.wikipedia.org/wiki/Supervised_learning) algorithm. Consider given input data $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m \\in \\mathbb{R}^n$ and output data $\\boldsymbol{y} = (y_1, \\dots, y_n) \\in \\mathbb{R}^n$, where the output data satisfies: $y_{i} \\in \\{-1, 1\\}$ for $1 \\leq i \\leq n$. We write\n",
    "$$\\boldsymbol{x}_j = (x_{1j}, \\dots, x_{nj}) = \\begin{bmatrix}\n",
    "x_{1j} \\\\\n",
    "\\vdots \\\\\n",
    "x_{nj}\n",
    "\\end{bmatrix}$$\n",
    "for $1 \\leq j \\leq m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly Separable Data\n",
    "\n",
    "We say that $((\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m), \\boldsymbol{y})$ is **linealy separable** if there exists an affine hyperplane $H \\subset \\mathbb{R}^m$ where we can label the two connected components of $\\mathbb{R}^m \\setminus H$ as $C_{1}$ and $C_{-1}$ such that\n",
    "$$\\boldsymbol{x}^{(i)} = (x_{i1}, \\dots, x_{im})^T = \\begin{bmatrix}\n",
    "x_{i1} & \\cdots & x_{im}\n",
    "\\end{bmatrix} \\text{ is in } \\left\\{\n",
    "\\begin{array}{ll}\n",
    "C_1 & \\text{ if } y_i = 1 \\text{ and} \\\\\n",
    "C_{-1} & \\text{ if } y_i = -1,\n",
    "\\end{array}\\right.$$\n",
    "for $1 \\leq i \\leq n$, where we consider $\\boldsymbol{x}^{(i)}$ as an $1 \\times m$ row matrix.\n",
    "\n",
    "**Remark**. Note that $m$ is the number of features and $n$ is the number of attribues, so it makes more sense to say that $(\\boldsymbol{x}^{(1)}, y_1), \\dots, (\\boldsymbol{x}^{(n)}, y_n)$ is linearly separable instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that there exists \n",
    "$$\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_m) = \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{m+1}$$ \n",
    "such that\n",
    "$$\\begin{bmatrix} 1 & \\boldsymbol{x}^{(i)} \\end{bmatrix}\\boldsymbol{\\beta} = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im} \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\geq 1 & \\text{ if } y_i = 1 \\text{ and} \\\\\n",
    "\\leq -1 & \\text{ if } y_i = -1\n",
    "\\end{array}\\right.$$\n",
    "for $1 \\leq i \\leq n$, or equivalently, we have\n",
    "\n",
    "$$y_i(\\beta_0 + \\boldsymbol{x}^{(i)}\\boldsymbol{\\beta}') = y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) \\geq 1$$ \n",
    "\n",
    "for $1 \\leq i \\leq n$, where $\\boldsymbol{\\beta}' = (\\beta_1, \\dots, \\beta_m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the distance between $C_1$ and $C_{-1}$ is at least the distance between the following two hyperplanes:\n",
    "* $H_{\\boldsymbol{\\beta},1}: \\beta_0 + \\beta_1 t_{1} + \\cdots + \\beta_m t_{m} = 1$ and\n",
    "* $H_{\\boldsymbol{\\beta},-1} : \\beta_0 + \\beta_1 t_{1} + \\cdots + \\beta_m t_{m} = -1$\n",
    "in $\\mathbb{R}^{m}$, which is equal to $2/\\sqrt{\\beta_1^2 + \\cdots + \\beta_m^2} = 2/\\|\\boldsymbol{\\beta}'\\|$.\n",
    "\n",
    "Noticing that our data is discrete, we may choose $\\boldsymbol{\\beta}$ such that $H_{\\boldsymbol{\\beta},1} = H_{1}$ and $H_{\\boldsymbol{\\beta},-1}$. The **margin** is defined to be the distance between the two connected components, which is exactly $2/\\sqrt{\\beta_1^2 + \\cdots + \\beta_m^2}$ if we keep the same notation. Hence, maximizing the margin is equivalent to minimize $\\|\\boldsymbol{\\beta}'\\|^2 = \\beta_1^2 + \\cdots + \\beta_m^2$ by choosing $\\beta_0, \\beta_1, \\dots, \\beta_m \\in \\mathbb{R}$ subject to the following conditions:\n",
    "$$y_i(\\beta_0 + \\boldsymbol{x}^{(i)}\\boldsymbol{\\beta}') = y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) \\geq 1$$ \n",
    "\n",
    "for $1 \\leq i \\leq n$.\n",
    "\n",
    "**Remark**. The [reference](https://hastie.su.domains/ElemStatLearn/) uses column-friendly notation for $\\boldsymbol{x}^{(i)}$ instead of row-friendly notation as in this document. The reason for our choice in row-friendly notation is to be consistent with the previous documents about linear and polynomial regeressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Use\n",
    "\n",
    "To practically use this algorithm, we first find $H_{\\boldsymbol{\\beta},1}$ and $H_{\\boldsymbol{\\beta},-1}$ when our data $((\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m), \\boldsymbol{y})$ is the training data. For this training data, we note that\n",
    "* the number of attribues is $n$, and\n",
    "* the number of features is $m$.\n",
    "\n",
    "In particular, any new data need to have $m$ features. For new input data, say $(t_1, \\dots, t_m)$, we consider\n",
    "$$L_{\\boldsymbol{\\beta}}(t_1, \\dots, t_m) := \\beta_0 + \\beta_1 t_1 + \\cdots + \\beta_m t_{m}.$$\n",
    "\n",
    "If $L_{\\boldsymbol{\\beta}}(t_1, \\dots, t_m) \\geq 1$, then we predict that the corresponding output data is $1$ and if $L_{\\boldsymbol{\\beta}}(t_1, \\dots, t_m)\\leq -1$, we predict that the corresponding output data is $-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linearly Separable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, our data $((\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m), \\boldsymbol{y})$, or equivalently $(\\boldsymbol{x}^{(1)}, y_1), \\dots, (\\boldsymbol{x}^{(n)}, y_n)$, may not be separable in $\\mathbb{R}^m$. (Even when $m = 2$, one can easily make an example.) In this case, for each $1 \\leq i \\leq n$, we consider the hyperplanes $H_{\\boldsymbol{\\beta},1-\\xi_i}$ and $H_{\\boldsymbol{\\beta},\\xi_i-1}$, where $H_{\\beta, a}$ is defined by $\\beta_0 + \\beta_1 t_1 + \\cdots + \\beta_m t_m = a$. Choice of $\\boldsymbol{\\xi} = (\\xi_1, \\dots, \\xi_n)$ with $\\xi_i \\geq 0$ let us shift our hyperplans so that they can classify non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to choose $\\beta_0, \\beta_1, \\dots, \\beta_m$ so that\n",
    "$$\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im} \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\geq 1 - \\xi_i & \\text{ if } y_i = 1 \\text{ and} \\\\\n",
    "\\leq \\xi_i - 1 & \\text{ if } y_i = - 1,\n",
    "\\end{array}\\right.$$\n",
    "or equivalently, we want\n",
    "\n",
    "$$y_i(\\beta_0 + \\boldsymbol{x}^{(i)}\\boldsymbol{\\beta}') = y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) \\geq 1 - \\xi_i$$ \n",
    "\n",
    "for $1 \\leq i \\leq n$. We also want to choose $\\beta_i$'s in the way that the distance $2/\\|\\boldsymbol{\\beta}\\|$ between each pair $(H_{\\boldsymbol{\\beta},1-\\xi_i}, H_{\\boldsymbol{\\beta},\\xi_i-1})$ of hyperplanes is maximized, or equivalently, we want $|\\boldsymbol{\\beta}\\|$ to be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we are allowed to choose any $\\xi_1, \\dots, \\xi_n \\geq 0$, then it is possible to classify the given data $(\\boldsymbol{x}^{(1)}, y_1), \\dots, (\\boldsymbol{x}^{(n)}, y_n)$, but the hyperplanes we construct may not be so useful for the new data that is not part of the given data. Hence, we want to make sure that $\\xi_1, \\dots, \\xi_n \\geq 0$ are chosen with some constraint. We fix a constant $C > 0$ and minimize\n",
    "$$(\\beta_1^2 + \\cdots + \\beta_m^2)/2 + C(\\xi_1 + \\cdots + \\xi_n)$$\n",
    "such that\n",
    "$$y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) \\geq 1 - \\xi_i$$\n",
    "for $1 \\leq i \\leq n$ by choosing $\\beta_0, \\beta_1, \\dots, \\beta_m \\in \\mathbb{R}$ and $\\xi_1, \\dots, \\xi_n \\geq 0$. Note that the bigger $C$ is, the more constraintes that we impose on $\\xi_1, \\dots, \\xi_n \\geq 0$. We also allow $C = \\infty$, which corresponds to $\\xi_1 = \\cdots = \\xi_n = 0$, recovering the linearly separable case.\n",
    "\n",
    "**Remark**. The reason for taking $1/2$ of the sum of squares of $\\beta_i$ above is to make it easy to differentiate in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we want to choose $\\beta_0, \\beta_1, \\dots, \\beta_m, \\xi_1, \\dots, \\xi_n \\in \\mathbb{R}$ that minimize\n",
    "$$f(\\beta_1, \\dots, \\beta_m, \\xi_1, \\dots, \\xi_n) := (\\beta_1^2 + \\cdots + \\beta_m^2)/2 + C(\\xi_1 + \\cdots + \\xi_n)$$\n",
    "with the following contraints:\n",
    "\n",
    "* $\\xi_1, \\dots, \\xi_n \\geq 0$;\n",
    "* $g_i(\\beta_0, \\beta_1, \\dots, \\beta_m, \\xi_1, \\dots, \\xi_n) := y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) - (1 - \\xi_i) \\geq 0$ for $1 \\leq i \\leq n$.\n",
    "\n",
    "Using [Lagrange multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier), we note that at such $(\\beta_0, \\beta_1, \\dots, \\beta_m, \\xi_1, \\dots, \\xi_n)$, we must have some $\\alpha_1, \\dots, \\alpha_n, \\mu_1, \\dots, \\mu_n \\in \\mathbb{R}$ such that\n",
    "$$\\nabla f(\\beta_1, \\dots, \\beta_m, \\xi_1, \\dots, \\xi_n) = \\mu_1 \\boldsymbol{e}_{n+1} + \\cdots + \\mu_n \\boldsymbol{e}_{2n} + \\sum_{i=1}^n \\alpha_i \\nabla g_i(\\beta_0, \\beta_1, \\dots, \\beta_m, \\xi_1, \\dots, \\xi_n)$$\n",
    "which gives us\n",
    "* $\\alpha_i = C - \\mu_i$ for $1 \\leq i \\leq n$;\n",
    "* $\\alpha_1y_1 + \\cdots + \\alpha_ny_n = 0$;\n",
    "* $\\beta_j = \\sum_{i=1}^n\\alpha_i y_i x_{ij}$ for $1 \\leq j \\leq m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the auxiliary function\n",
    "$$\\begin{align*}\n",
    "&L(\\beta_0, \\beta_1, \\dots, \\beta_m, \\xi_1, \\dots, \\xi_n, \\alpha_1, \\dots, \\alpha_n, \\mu_1, \\dots, \\mu_n) \\\\\n",
    "&:= f(\\beta_1, \\dots, \\beta_m, \\xi_1, \\dots, \\xi_n) - \\sum_{i=1}^n (\\alpha_i g_i(\\beta_0, \\beta_1, \\dots, \\beta_m, \\xi_1, \\dots, \\xi_n) + \\mu_i\\xi_i),\n",
    "\\end{align*}$$\n",
    "which is often called the **Lagrangian function**.\n",
    "As $f$ and $g_i$'s are [convex](https://en.wikipedia.org/wiki/Convex_function), we can apply the [Karush--Kuhn--Tucker theorem](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#cite_note-2), which gurantees such a minimum at a guaranteed saddle point of $L$ with the condtions $\\alpha_i, \\mu_i \\geq 0$, which implies $0 \\leq \\alpha_i \\leq C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, there is a set of necessary conditions called **completmentary slackness**, which guarantees the minimum. These conditions in our situation are:\n",
    "* $\\alpha_i(y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) - (1 - \\xi_i)) = 0$ for $1 \\leq i \\leq n$;\n",
    "* $\\mu_i \\xi_i = 0$ for $1 \\leq i \\leq n$.\n",
    "\n",
    "These extra conditions create more constraints for a practical algorithm for locating a solution to our minimization problem.\n",
    "\n",
    "**Remark**. For derivation of these conditions, see [Sections 5.4.2 and 5.5.2 of Boyd and Vandenberghe](https://web.stanford.edu/~boyd/cvxbook/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we apply all the given conditions to $L$, we end up having\n",
    "$$L = f = \\alpha_1 + \\cdots + \\alpha_n - \\frac{1}{2}\\sum_{i=1}^n\\sum_{l=1}^n\\alpha_i\\alpha_l y_i y_l \\langle \\boldsymbol{x}^{(i)}, \\boldsymbol{x}^{(l)} \\rangle,$$\n",
    "which is what we are supposed to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the trick is to embed each of $\\boldsymbol{x}^{(1)}, \\dots, \\boldsymbol{x}^{(n)}$ into a higher-dimensional vector space $V$, via some embedding $\\varphi : \\mathbb{R}^m \\hookrightarrow V$ such that $\\varphi(\\boldsymbol{x}^{(1)}), \\dots, \\varphi(\\boldsymbol{x}^{(n)})$ are linearly separable in $V$. Surprisingly, even in practice, the vector space $V$ may be infinitely-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, given $\\boldsymbol{t} = (t_1, \\dots, t_m) \\in \\mathbb{R}^m$, we take\n",
    "$$V = \\mathbb{R}^{\\infty} = \\mathbb{R} \\times \\mathbb{R} \\times \\mathbb{R} \\times \\cdots$$\n",
    "and define\n",
    "$$\\varphi(\\boldsymbol{t}) := \\exp\\left(-\\frac{1}{2}\\|\\boldsymbol{t}\\|^2\\right)(a_{0,l_{m,0}}(\\boldsymbol{t}), a_{1, 1}(\\boldsymbol{t}), \\dots, a_{1, l_{m,1}}(\\boldsymbol{t}), a_{2, 1}(\\boldsymbol{t}), \\dots),$$\n",
    "where\n",
    "\n",
    "* $l_{m,j} := {m + j - 1 \\choose j}$ and\n",
    "* $a_{j,l} := \\frac{x^{n_1} \\cdots x_m^{n_m}}{(n_1! \\cdots n_k!)^{1/2}}$,\n",
    "\n",
    "with $n_1 + \\cdots + n_m = j$ (a partition of $j$) and $1 \\leq l \\leq l_{m,j}$ means the order of such a partition. Magically, [it turns out that](https://en.wikipedia.org/wiki/Radial_basis_function_kernel) for any $\\boldsymbol{s}$ and $\\boldsymbol{t}$ in $V$, with the standard dot product, we have\n",
    "$$\\langle \\varphi(\\boldsymbol{s}), \\varphi(\\boldsymbol{t}) \\rangle = \\exp\\left(-\\frac{1}{2}\\|\\boldsymbol{s} - \\boldsymbol{t}\\|^2\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral is that even if we work on our classfication in a very high-dimentional space $V$ (although we actually work in a finite subspace of it), all we need to do is to consider\n",
    "$$L = f = \\alpha_1 + \\cdots + \\alpha_n - \\frac{1}{2}\\sum_{i=1}^n\\sum_{l=1}^n\\alpha_i\\alpha_l y_i y_l \\langle \\boldsymbol{x}^{(i)}, \\boldsymbol{x}^{(l)} \\rangle,$$\n",
    "and replace $\\langle \\boldsymbol{x}^{(i)}, \\boldsymbol{x}^{(l)} \\rangle$ with\n",
    "$$\\langle \\varphi(\\boldsymbol{x}^{(i)}), \\varphi(\\boldsymbol{x}^{(l)}) \\rangle = \\exp\\left(-\\frac{1}{2}\\|\\boldsymbol{x}^{(i)} - \\boldsymbol{x}^{(l)}\\|^2\\right)$$\n",
    "without having to think about what exactly $\\varphi(\\boldsymbol{x}^{(i)})$ is! That is, we minimize\n",
    "$$\\alpha_1 + \\cdots + \\alpha_n - \\frac{1}{2}\\sum_{i=1}^n\\sum_{l=1}^n\\alpha_i\\alpha_l y_i y_l \\exp\\left(-\\frac{1}{2}\\|\\boldsymbol{x}^{(i)} - \\boldsymbol{x}^{(l)}\\|^2\\right)$$\n",
    "which is certainly more doable (with other constraints discussed above).\n",
    "\n",
    "The map\n",
    "$$(\\boldsymbol{s}, \\boldsymbol{t}) \\mapsto \\exp\\left(-\\frac{1}{2}\\|\\boldsymbol{s} - \\boldsymbol{t}\\|^2\\right)$$\n",
    "is called the **radial basis function kernel** and $\\varphi$ is called the **feature map** for such a kernel. There are other kinds of kernels that are available for similar purpose of SVM, but we shall discuss them more when we need them."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
