{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S5$. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: [Gilyoung Cheong](https://www.linkedin.com/in/gycheong/)\n",
    "\n",
    "**References**\n",
    "* [Wikipedia page about support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "* [\"The Elements of Statistical Learning\" by Hastie, Tibshirani, Friedman](https://hastie.su.domains/ElemStatLearn/): Chapter 12\n",
    "* [Wikipedia page about radial basis function kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)\n",
    "* [\"Convex Optimization\" by Boyd and Vandenberghe](https://web.stanford.edu/~boyd/cvxbook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machine is a [supervised machine learning](https://en.wikipedia.org/wiki/Supervised_learning) algorithm for classification. Consider given input data $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m \\in \\mathbb{R}^n$ and output data $\\boldsymbol{y} = (y_1, \\dots, y_n) \\in \\mathbb{R}^n$, where the output data satisfies: $y_{i} \\in \\{-1, 1\\}$ for $1 \\leq i \\leq n$. We write\n",
    "$$\\boldsymbol{x}_j = (x_{1j}, \\dots, x_{nj}) = \\begin{bmatrix}\n",
    "x_{1j} \\\\\n",
    "\\vdots \\\\\n",
    "x_{nj}\n",
    "\\end{bmatrix}$$\n",
    "for $1 \\leq j \\leq m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convention**. We shall write\n",
    "$$\\boldsymbol{x}^{(i)} = (x_{i1}, \\dots, x_{im})^T = \\begin{bmatrix}\n",
    "x_{i1} & \\cdots & x_{im}\n",
    "\\end{bmatrix}$$\n",
    "for $1 \\leq i \\leq n$, where we consider $\\boldsymbol{x}^{(i)}$ as an $1 \\times m$ row matrix. This is because we consider\n",
    "\n",
    "* $n$ as the number of indices of the input data and\n",
    "* $m$ as the number of features of the input data.\n",
    "\n",
    "This way, we consider our data \n",
    "\n",
    "$$((\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m), \\boldsymbol{y})$$\n",
    "\n",
    "in the form $(\\boldsymbol{x}^{(1)}, y_1), \\dots, (\\boldsymbol{x}^{(n)}, y_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly Separable Data\n",
    "\n",
    "We say that $(\\boldsymbol{x}^{(1)}, y_1), \\dots, (\\boldsymbol{x}^{(n)}, y_n)$ are **linealy separable** if there exists an affine hyperplane $H \\subset \\mathbb{R}^m$ where we can label the two connected components of $\\mathbb{R}^m \\setminus H$ as $C_{1}$ and $C_{-1}$ such that\n",
    "\n",
    "$$\\boldsymbol{x}^{(i)} = \\begin{bmatrix}\n",
    "x_{i1} & \\cdots & x_{im}\n",
    "\\end{bmatrix} \\text{ is in } \\left\\{\n",
    "\\begin{array}{ll}\n",
    "C_1 & \\text{ if } y_i = 1 \\text{ and} \\\\\n",
    "C_{-1} & \\text{ if } y_i = -1.\n",
    "\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the amount of the given data is finite, this implies that there exists \n",
    "$$\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_m) = \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{m+1}$$ \n",
    "such that\n",
    "$$\\begin{bmatrix} 1 & \\boldsymbol{x}^{(i)} \\end{bmatrix}\\boldsymbol{\\beta} = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im} \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\geq 1 & \\text{ if } y_i = 1 \\text{ and} \\\\\n",
    "\\leq -1 & \\text{ if } y_i = -1\n",
    "\\end{array}\\right.$$\n",
    "for $1 \\leq i \\leq n$, or equivalently, we have\n",
    "\n",
    "$$y_i(\\beta_0 + \\boldsymbol{x}^{(i)}\\boldsymbol{\\beta}') = y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) \\geq 1$$ \n",
    "\n",
    "for $1 \\leq i \\leq n$, where $\\boldsymbol{\\beta}' = (\\beta_1, \\dots, \\beta_m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the distance between two separated input datasets is $\\geq 2/\\sqrt{\\beta_1^2 + \\cdots + \\beta_m^2} = 2/\\|\\boldsymbol{\\beta}'\\|$, which is the distance between the following two hyperplanes:\n",
    "* $H_{\\boldsymbol{\\beta},1}: \\beta_0 + \\beta_1 t_{1} + \\cdots + \\beta_m t_{m} = 1$ and\n",
    "* $H_{\\boldsymbol{\\beta},-1} : \\beta_0 + \\beta_1 t_{1} + \\cdots + \\beta_m t_{m} = -1$\n",
    "in $\\mathbb{R}^{m}$.\n",
    "\n",
    "Noticing that our data is discrete, we may choose $\\boldsymbol{\\beta}$ such that the distance between the two sets is equal to the distance between $H_{\\boldsymbol{\\beta},1}$ and $H_{\\boldsymbol{\\beta},-1}$. The **margin** is defined to be the distance between the two separated input datasets, which is exactly $2/\\sqrt{\\beta_1^2 + \\cdots + \\beta_m^2}$ if we keep the same notation. Hence, maximizing the margin is equivalent to minimize $\\|\\boldsymbol{\\beta}'\\|^2 = \\beta_1^2 + \\cdots + \\beta_m^2$ by choosing $\\beta_0, \\beta_1, \\dots, \\beta_m \\in \\mathbb{R}$ subject to the following conditions:\n",
    "$$y_i(\\beta_0 + \\boldsymbol{x}^{(i)}\\boldsymbol{\\beta}') = y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) \\geq 1$$ \n",
    "\n",
    "for $1 \\leq i \\leq n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Training for Linearly Separable Data\n",
    "\n",
    "To practically use this algorithm, we first need to find $\\boldsymbol{\\beta} \\in \\mathbb{R}^{m+1}$ that comes with the hyperplanes $H_{\\boldsymbol{\\beta},1}$ and $H_{\\boldsymbol{\\beta},-1}$ described above when our data $((\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m), \\boldsymbol{y}) = ((\\boldsymbol{x}^{(1)}, y_1), \\dots, (\\boldsymbol{x}^{(n)}, y_n))$ is the training data, which is assumed to be linearly separable. For this training data, we note that\n",
    "* the number of attribues is $n$, and\n",
    "* the number of features is $m$.\n",
    "\n",
    "In particular, any new data need to have $m$ features. For new input data, say $(t_1, \\dots, t_m)$, we consider\n",
    "$$L_{\\boldsymbol{\\beta}}(t_1, \\dots, t_m) := \\beta_0 + \\beta_1 t_1 + \\cdots + \\beta_m t_{m}.$$\n",
    "\n",
    "If $L_{\\boldsymbol{\\beta}}(t_1, \\dots, t_m) \\geq 1$, then we predict that the corresponding output data is $1$ and if $L_{\\boldsymbol{\\beta}}(t_1, \\dots, t_m)\\leq -1$, we predict that the corresponding output data is $-1$. However, we may also exhibit the cases inbetween: \n",
    "$$-1 < L_{\\boldsymbol{\\beta}}(t_1, \\dots, t_m) < 1.$$\n",
    "\n",
    "How should we deal with these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss\n",
    "\n",
    "For each index $i$, we introduce the $i$-th **hinge loss**\n",
    "$$h_i(\\beta_0, \\beta_1, \\dots, \\beta_m) := \\max(0, 1 - y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im})).$$\n",
    "\n",
    "Note that $h_i(\\beta_0, \\beta_1, \\dots, \\beta_m) = 0$ if and only if\n",
    "$$y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im})) \\geq 1.$$\n",
    "\n",
    "When $h_i(\\beta_0, \\beta_1, \\dots, \\beta_m) \\neq 0$, we have\n",
    "$$h_i(\\beta_0, \\beta_1, \\dots, \\beta_m) = 1 - y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) > 0,$$\n",
    "\n",
    "and note that the [distance](https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_plane#:~:text=In%20Euclidean%20space%2C%20the%20distance,nearest%20point%20on%20the%20plane.) from the point $\\boldsymbol{x}^{(i)} = (x_{i1}, \\dots, x_{im})$ to the plane \n",
    "$$\\{(t_1, \\dots, t_m) \\in \\mathbb{R}^m : \\beta_0 + \\beta_1 t_1 + \\cdots + \\beta_m t_{m} = 1\\}$$\n",
    "\n",
    "is equal to\n",
    "\n",
    "$$\\frac{1 - y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im})}{\\|\\boldsymbol{\\beta}\\|} = \\frac{h_i(\\boldsymbol{\\beta})}{\\|\\boldsymbol{\\beta}\\|}.$$\n",
    "\n",
    "In other words, we have:\n",
    "\n",
    "**Lemma**. The $i$-th hinge loss $h_i(\\boldsymbol{\\beta})$ is equal to $\\|\\boldsymbol{\\beta}\\|$ times the distance from $\\boldsymbol{x}^{(i)}$ to the plane\n",
    "$$\\{(t_1, \\dots, t_m) \\in \\mathbb{R}^m : \\beta_0 + \\beta_1 t_1 + \\cdots + \\beta_m t_{m} = 1\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearly Separable Data with Hinge Loss\n",
    "\n",
    "In the case of linearly separable data, there exists $\\boldsymbol{\\beta} \\in \\mathbb{R}^{m+1}$ that minimizes $\\|\\boldsymbol{\\beta}\\|$ and any nonzero $i$-th hinge loss satisfies \n",
    "$$h_i(\\boldsymbol{\\beta}) \\geq \\|\\boldsymbol{\\beta}\\| \\cdot 2/\\|\\boldsymbol{\\beta}\\| = 2.$$\n",
    "\n",
    "such that for each $1 \\leq i \\leq n$. Now, we would like to say that even for new input data $(t_1, \\dots, t_m) \\in \\mathbb{R}^m$, if $\\beta_0 + \\beta_1 t_1 + \\cdots + \\beta_m t_{m} < 1$, then we predict the output as failure (i.e., $-1$). In order to do this, in the training step (i.e., when we are picking $\\boldsymbol{\\beta}$), we need to make sure that the nonzero hinge losses are not too big. We then can formulate this as the following optimization question:\n",
    "\n",
    "* find $\\boldsymbol{\\beta} \\in \\mathbb{R}^{m+1}$ that minimizes $\\frac{1}{2}\\|\\boldsymbol{\\beta}\\|^2 + C (h_1(\\boldsymbol{\\beta}) + \\cdots + h_n(\\boldsymbol{\\beta}))$,\n",
    "\n",
    "where $C \\geq 0$ is a constant. Of course $C = 0$ is identical to the previous discussion, so it is only meaningful when we consider $C > 0$.\n",
    "\n",
    "Note that this optimization problem is identical to the following:\n",
    "\n",
    "* find $\\boldsymbol{\\beta} \\in \\mathbb{R}^{m+1}$ and $\\boldsymbol{\\xi} \\in \\mathbb{R}^n$ that minimizes $\\frac{1}{2}\\|\\boldsymbol{\\beta}\\|^2 + C (\\xi_1 + \\cdots + \\xi_n)$ subject to\n",
    "* $\\xi_1, \\dots, \\xi_n \\geq 0$ and\n",
    "* $y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) \\geq 1 - \\xi_i$ for $1 \\leq i \\leq n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linearly Separable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data $((\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m), \\boldsymbol{y})$, or equivalently $(\\boldsymbol{x}^{(1)}, y_1), \\dots, (\\boldsymbol{x}^{(n)}, y_n)$, may not be linearly separable in $\\mathbb{R}^m$. (Even when $m = 2$, one can easily make an example.) In this case, for each $1 \\leq i \\leq n$, we consider the hyperplanes $H_{\\boldsymbol{\\beta},1-\\xi_i}$ and $H_{\\boldsymbol{\\beta},\\xi_i-1}$, where $H_{\\beta, a}$ is defined by $\\beta_0 + \\beta_1 t_1 + \\cdots + \\beta_m t_m = a$. Choice of $\\boldsymbol{\\xi} = (\\xi_1, \\dots, \\xi_n)$ with $\\xi_i \\geq 0$ let us shift our hyperplans so that they can classify non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to choose\n",
    "\n",
    "* $\\beta_0, \\beta_1, \\dots, \\beta_m \\in \\mathbb{R}$ and\n",
    "* $\\xi_1, \\dots, \\xi_n \\in \\mathbb{R}_{\\geq 0}$ \n",
    "\n",
    "so that\n",
    "$$\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im} \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\geq 1 - \\xi_i & \\text{ if } y_i = 1 \\text{ and} \\\\\n",
    "\\leq \\xi_i - 1 & \\text{ if } y_i = - 1,\n",
    "\\end{array}\\right.$$\n",
    "or equivalently, we want\n",
    "\n",
    "$$y_i(\\beta_0 + \\boldsymbol{x}^{(i)}\\boldsymbol{\\beta}') = y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) \\geq 1 - \\xi_i$$ \n",
    "\n",
    "for $1 \\leq i \\leq n$. We also want to choose $\\beta_i$'s in the way that the distance $2/\\|\\boldsymbol{\\beta}\\|$ between each pair $(H_{\\boldsymbol{\\beta},1-\\xi_i}, H_{\\boldsymbol{\\beta},\\xi_i-1})$ of hyperplanes is maximized, or equivalently, we want $\\|\\boldsymbol{\\beta}\\|$ to be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we are allowed to choose any $\\xi_1, \\dots, \\xi_n \\geq 0$, then it is possible to classify the given data $(\\boldsymbol{x}^{(1)}, y_1), \\dots, (\\boldsymbol{x}^{(n)}, y_n)$, but the hyperplanes we construct may not be so useful for the new data that is not part of the given data. Hence, we want to make sure that $\\xi_1, \\dots, \\xi_n \\geq 0$ are chosen with some constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our optimization problem is to\n",
    "* minimize $\\frac{1}{2}(\\beta_1^2 + \\cdots + \\beta_m^2) + C(\\xi_1 + \\cdots + \\xi_n)$ subject to\n",
    "* $y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) \\geq 1 - \\xi_i$ for $1 \\leq i \\leq n$ and\n",
    "* $\\xi_1, \\dots, \\xi_n \\geq 0$,\n",
    "\n",
    "for some constant $C > 0$. As $C$ is closer to $0$, we have less flexibility, but the model we get (if we succeed) would be better. This is identical to the optimization problem derived with the hinge losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the conditions are given by convex functions, and we may check that our problem satisfies [Slater's condition](https://github.com/gycheong/machine_learning/blob/main/ML%20theory/3%20Lagrange%20duality%20and%20Slater's%20condition.ipynb), so we can use the fact that there is zero duality gap. The Lagrangian of the optimization problem is\n",
    "\n",
    "$$L(\\boldsymbol{\\beta}, \\boldsymbol{\\xi}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = \\frac{1}{2}(\\beta_1^2 + \\cdots + \\beta_m^2) + C(\\xi_1 + \\cdots + \\xi_n) + \\sum_{i=1}^{n}[\\lambda_i(1 - \\xi_i - y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im})) - \\nu_i \\xi_i]$$\n",
    "\n",
    "and to get the Lagrangian dual function we need to minimze $L$ over $\\boldsymbol{\\beta}, \\boldsymbol{\\xi}$ fixing other variables in $\\boldsymbol{\\lambda}, \\boldsymbol{\\nu} \\geq 0$. We have\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial \\beta_j} = \\beta_j - \\sum_{i=1}^{n}\\lambda_i y_i x_{ij}$ for $1 \\leq j \\leq m$,\n",
    "* $\\frac{\\partial L}{\\partial \\beta_0} = - \\sum_{i=1}^{n}\\lambda_i y_i$, and\n",
    "* $\\frac{\\partial L}{\\partial \\xi_i} = C - \\lambda_i - \\nu_i$ for $1 \\leq i \\leq n$.\n",
    "\n",
    "Hence, at any feasible point $(\\boldsymbol{\\beta}, \\boldsymbol{\\xi})$ of minimiztion, we must have\n",
    "\n",
    "* $\\beta_j = \\sum_{i=1}^{n}\\lambda_i y_i x_{ij}$ for $1 \\leq j \\leq m$,\n",
    "* $\\sum_{i=1}^{n}\\lambda_i y_i = 0$, and\n",
    "* $C = \\lambda_i + \\nu_i$ for $1 \\leq i \\leq n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Substitutions give us the Lagrange dual function as follows:\n",
    "\n",
    "$$g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = g(\\boldsymbol{\\lambda}) = \\lambda_1 + \\cdots + \\lambda_n - \\frac{1}{2}\\sum_{i=1}^n\\sum_{l=1}^n \\lambda_i\\lambda_{l}y_iy_{l}\\langle \\boldsymbol{x}^{(i)}, \\boldsymbol{x}^{(l)}\\rangle.$$\n",
    "\n",
    "We note that the last condition also gives $0 \\leq \\lambda_i \\leq C$ for $1 \\leq i \\leq n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Assumeing that $\\boldsymbol{\\lambda}, \\boldsymbol{\\nu} \\geq 0$ are chosen optimally, the [complementary slackness condition](https://github.com/gycheong/machine_learning/blob/main/ML%20theory/3%20Lagrange%20duality%20and%20Slater's%20condition.ipynb) gives us (at an optimal point $\\boldsymbol{\\beta} \\in \\mathbb{R}^{m+1}$)\n",
    "\n",
    "* $\\lambda_i (y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) - (1 - \\xi_i)) = 0$ and\n",
    "* $\\nu_i\\xi_i = 0$\n",
    "\n",
    "for $1 \\leq i \\leq n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, note that when determining $\\beta_1, \\dots, \\beta_m$, we have\n",
    "$$\\beta_j = \\sum_{i=1}^{n}\\lambda_i y_i x_{ij},$$\n",
    "\n",
    "for $1 \\leq j \\leq m$ and for any nonzero $\\lambda_i$, we must have\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) = 1 - \\xi_i.$$\n",
    "\n",
    "The indices $i$ for which $\\lambda_i \\neq 0$ are called **support vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if $\\xi_i \\neq 0$, then we get $\\nu_i = 0$ so that $\\lambda_i = C$. Continuing further, we see that $\\xi_i = 0$ if and only if $\\lambda_i < C$. If any $i$ has $\\lambda_i \\neq 0$ and $\\xi_i = 0$ (or equivalently $0 < \\lambda_i < C$), then \n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}) = 1,$$\n",
    "\n",
    "so noting $y_i = \\pm 1$, we have\n",
    "\n",
    "$$\\beta_0 = y_i -(\\beta_1 x_{i1} + \\cdots + \\beta_m x_{im}).$$\n",
    "\n",
    "The only thing that is left is to choose $\\boldsymbol{\\lambda} \\geq 0$ by maximziing the quadratic function\n",
    "\n",
    "$$g(\\boldsymbol{\\lambda}) = \\lambda_1 + \\cdots + \\lambda_n - \\frac{1}{2}\\sum_{i=1}^n\\sum_{l=1}^n \\lambda_i\\lambda_{l}y_iy_{l}\\langle \\boldsymbol{x}^{(i)}, \\boldsymbol{x}^{(l)}\\rangle.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [Hastie, Tibshirani, and Friedman](https://hastie.su.domains/ElemStatLearn/) (p.421), there is a standard way to do this, but we shall not dig into this now.\n",
    "\n",
    "### Decision\n",
    "\n",
    "Given $(t_1, \\dots, t_m) \\in \\mathbb{R}^m$, the decision is then made by the sign of\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\beta_0 + \\beta_1 t_1 + \\cdots + \\beta_m t_m &= \\beta_0 + \\sum_{\\substack{1 \\leq i \\leq n \\\\ i \\text{ support vector}}}y_i\\lambda_i \\sum_{j=1}^mx_{ij}t_j \\\\\n",
    "&= \\beta_0 + \\sum_{\\substack{1 \\leq i \\leq n \\\\ i \\text{ support vector}}}y_i\\lambda_i \\langle \\boldsymbol{x}^{(i)}, \\boldsymbol{t} \\rangle.\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Trick\n",
    "\n",
    "Lastly, we introduce a trick to reduce computational load.\n",
    "\n",
    "First, we embed each of $\\boldsymbol{x}^{(1)}, \\dots, \\boldsymbol{x}^{(n)}$ into a higher-dimensional vector space $V$, via some embedding $\\varphi : \\mathbb{R}^m \\hookrightarrow V$ such that $\\varphi(\\boldsymbol{x}^{(1)}), \\dots, \\varphi(\\boldsymbol{x}^{(n)})$ are linearly separable in $V$. Surprisingly, even in practice, the vector space $V$ may be infinitely-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, given $\\boldsymbol{t} = (t_1, \\dots, t_m) \\in \\mathbb{R}^m$, we take\n",
    "$$V = \\mathbb{R}^{\\infty} = \\mathbb{R} \\times \\mathbb{R} \\times \\mathbb{R} \\times \\cdots$$\n",
    "and define\n",
    "$$\\varphi(\\boldsymbol{t}) := \\exp\\left(-\\frac{1}{2}\\|\\boldsymbol{t}\\|^2\\right)(a_{0,l_{m,0}}(\\boldsymbol{t}), a_{1, 1}(\\boldsymbol{t}), \\dots, a_{1, l_{m,1}}(\\boldsymbol{t}), a_{2, 1}(\\boldsymbol{t}), \\dots),$$\n",
    "where\n",
    "\n",
    "* $l_{m,j} := {m + j - 1 \\choose j}$ and\n",
    "* $a_{j,l} := \\frac{x^{n_1} \\cdots x_m^{n_m}}{(n_1! \\cdots n_k!)^{1/2}}$,\n",
    "\n",
    "with $n_1 + \\cdots + n_m = j$ (a partition of $j$) and $1 \\leq l \\leq l_{m,j}$ means the order of such a partition. Magically, [it turns out that](https://en.wikipedia.org/wiki/Radial_basis_function_kernel) for any $\\boldsymbol{s}$ and $\\boldsymbol{t}$ in $V$, with the standard dot product, we have\n",
    "$$\\langle \\varphi(\\boldsymbol{s}), \\varphi(\\boldsymbol{t}) \\rangle = \\exp\\left(-\\frac{1}{2}\\|\\boldsymbol{s} - \\boldsymbol{t}\\|^2\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral is that even if we work on our classfication in a very high-dimentional space $V$ (although we actually work in a finite subspace of it), all we need to do is to consider maximizing\n",
    "$$g(\\boldsymbol{\\lambda}) = \\lambda_1 + \\cdots + \\lambda_n - \\frac{1}{2}\\sum_{i=1}^n\\sum_{l=1}^n \\lambda_i\\lambda_{l}y_iy_{l}\\langle \\boldsymbol{x}^{(i)}, \\boldsymbol{x}^{(l)}\\rangle.$$\n",
    "\n",
    "Now, replace $\\langle \\boldsymbol{x}^{(i)}, \\boldsymbol{x}^{(l)} \\rangle$ with\n",
    "$$\\langle \\varphi(\\boldsymbol{x}^{(i)}), \\varphi(\\boldsymbol{x}^{(l)}) \\rangle = \\exp\\left(-\\frac{1}{2}\\|\\boldsymbol{x}^{(i)} - \\boldsymbol{x}^{(l)}\\|^2\\right)$$\n",
    "without having to think about what exactly $\\varphi(\\boldsymbol{x}^{(i)})$ is! (Just think of $\\varphi$ as a new labeling of our data.) That is, we now just need to maximize\n",
    "$$\\lambda_1 + \\cdots + \\lambda_n - \\frac{1}{2}\\sum_{i=1}^n\\sum_{l=1}^n\\lambda_i\\lambda_l y_i y_l \\exp\\left(-\\frac{1}{2}\\|\\boldsymbol{x}^{(i)} - \\boldsymbol{x}^{(l)}\\|^2\\right)$$\n",
    "which is certainly more doable (with other constraints discussed above).\n",
    "\n",
    "The map\n",
    "$$(\\boldsymbol{s}, \\boldsymbol{t}) \\mapsto \\exp\\left(-\\frac{1}{2}\\|\\boldsymbol{s} - \\boldsymbol{t}\\|^2\\right)$$\n",
    "is called the **radial basis function kernel** and $\\varphi$ is called the **feature map** for such a kernel. There are other kinds of kernels that are available for similar purpose of SVM, but we shall discuss them more when we need them.\n",
    "\n",
    "### Decision\n",
    "\n",
    "Given $(t_1, \\dots, t_m) \\in \\mathbb{R}^m$, the decision is then made by the sign of\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\beta_0 + \\sum_{\\substack{1 \\leq i \\leq n \\\\ i \\text{ support vector}}}y_i\\lambda_i \\langle \\varphi(\\boldsymbol{x}^{(i)}), \\varphi(\\boldsymbol{t}) \\rangle = \\beta_0 + \\sum_{\\substack{1 \\leq i \\leq n \\\\ i \\text{ support vector}}}y_i\\lambda_i \\exp\\left(-\\frac{1}{2}\\|\\boldsymbol{x}^{(i)} - \\boldsymbol{t}\\|^2\\right).\n",
    "\\end{align*}$$\n",
    "\n",
    "**Remark**. The [scikit-learn SVC](https://scikit-learn.org/stable/modules/svm.html) also uses this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
