{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrange duality and Slater's condition\n",
    "\n",
    "**Author**: [Gilyoung Cheong](https://www.linkedin.com/in/gycheong/)\n",
    "\n",
    "**References**\n",
    "* [\"Convex Optimization\" by Boyd and Vandenberghe](https://web.stanford.edu/~boyd/cvxbook/) -- Chapter 5 of this excellent book is the main reference for this article.\n",
    "* [\"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman](https://hastie.su.domains/ElemStatLearn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange dual of an optimization problem\n",
    "\n",
    "Consider the following optimization problem for the real valued functions $f_0, f_1, \\dots, f_m, h_1, \\dots, h_k$ defined on $\\mathbb{R}^n$:\n",
    "* Minimize $f_0(\\boldsymbol{x})$ subject to \n",
    "* $f_1(\\boldsymbol{x}), \\dots, f_m(\\boldsymbol{x}) \\leq 0$ and\n",
    "* $h_1(\\boldsymbol{x}), \\dots, h_k(\\boldsymbol{x}) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work with a common domain $D \\subset \\mathbb{R}^n$ of $f_0, f_1, \\dots, f_m, h_1, \\dots, h_k$, and we assume that $D$ is nonempty. Depending on a situation, we may work with nicer $D$ such as an open subset of $\\mathbb{R}^n$ or a compact (i.e., closed and bounded) subset of $\\mathbb{R}^n$. The **Lagrangian** of the above optimization problem is a function $L : D \\times \\mathbb{R}^m \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}$ defined as\n",
    "$$L(\\boldsymbol{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) := f_0(\\boldsymbol{x}) + \\lambda_1 f_1(\\boldsymbol{x}) + \\cdots + \\lambda_m f_m(\\boldsymbol{x}) + \\nu_1 h_1(\\boldsymbol{x}) + \\cdots + \\nu_k h_k(\\boldsymbol{x}),$$\n",
    "the variables $\\lambda_1, \\dots, \\lambda_m, \\nu_1, \\dots, \\nu_k$ are callsed the **Lagrange multipliers** of the given constraints. The **Lagrange dual function** of the problem is a function $g : \\mathbb{R}^m \\times \\mathbb{R}^k$ defined as\n",
    "$$g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) := \\inf_{\\boldsymbol{x} \\in D}L(\\boldsymbol{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say $\\boldsymbol{x} \\in \\mathbb{R}^n$ is **feasible** if\n",
    "* $\\boldsymbol{x} \\in D$,\n",
    "* $f_1(\\boldsymbol{x}), \\dots, f_m(\\boldsymbol{x}) \\leq 0$, and\n",
    "* $h_1(\\boldsymbol{x}), \\dots, h_k(\\boldsymbol{x}) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma (Weak duality)**. If $\\boldsymbol{x} \\in D$ is a feasible to the optimization problem, then we must have\n",
    "$$g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) \\leq f_0(\\boldsymbol{x})$$\n",
    "for any $\\boldsymbol{\\lambda} = (\\lambda_1, \\dots, \\lambda_m)$ such that $\\lambda_1, \\dots, \\lambda_m \\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof*. We have $f_1(\\boldsymbol{x}), \\dots, f_m(\\boldsymbol{x}) \\leq 0$ and $h_1(\\boldsymbol{x}) = \\cdots = h_k(\\boldsymbol{x}) = 0$. For any $\\lambda_1, \\dots, \\lambda_m \\geq 0$ and $\\boldsymbol{\\nu} \\in \\mathbb{R}^k$, we have\n",
    "$$L(\\boldsymbol{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = f_0(\\boldsymbol{x}) + \\lambda_1 f_1(\\boldsymbol{x}) + \\cdots + \\lambda_m f_m(\\boldsymbol{x}) \\leq f_0(\\boldsymbol{x}),$$\n",
    "as claimed. This finishes the proof. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemma suggests that we can try to reach our minimum $f_0(\\boldsymbol{x}^*)$ by maximizing $g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$ under the condition $\\boldsymbol{\\lambda} \\geq \\boldsymbol{0}$ if such minimum exists, where the inequality between vectors is defined component-wise. The **Lagrange dual problem** of our original problem is the following.\n",
    "\n",
    "* Maximimze $g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$ subject to\n",
    "* $\\boldsymbol{\\lambda} \\geq \\boldsymbol{0}$.\n",
    "\n",
    "If $\\boldsymbol{x}^*$ is an optimal solution to our original problem and $(\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*)$ is an optimal solution to its Lagrange dual problem (with $\\boldsymbol{\\lambda}^* \\geq \\boldsymbol{0}$), the nonnegative real number $f_0(\\boldsymbol{x}^*) - g(\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*) \\geq 0$ is called the **optimal duality gap** of our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**. We consider the case where \n",
    "* $n = m$,\n",
    "* $f_0(\\boldsymbol{x})) = c_1x_1 + \\cdots + c_nx_n$,\n",
    "* $f_j(\\boldsymbol{x}) = -x_j$ for $1 \\leq j \\leq n$ and\n",
    "* $h_i(\\boldsymbol{x}) = a_{i1}x_1 + \\cdots + a_{in}x_n - b_i$ for $1 \\leq i \\leq k$,\n",
    "\n",
    "where $b_i, c_j, a_{ij} \\in \\mathbb{R}$ are given constants. That is, our problem is to\n",
    "\n",
    "* maximize $c_1x_1 + \\cdots + c_nx_n$ subject to\n",
    "* $x_1, \\dots, x_n \\geq 0$ and\n",
    "* $A \\boldsymbol{x} = \\boldsymbol{b}$,\n",
    "\n",
    "where $A = (a_{ij})_{i,j=1}^{k,n}$ and $\\boldsymbol{b} = (b_1, \\dots, b_n)$.\n",
    "\n",
    "In this case, we can take $D = \\mathbb{R}^n$ and the Lagrangian is\n",
    "\n",
    "$$L(\\boldsymbol{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = -b_1\\nu_1 - \\cdots - b_n\\nu_n + \\sum_{j=1}^n x_j(a_{1j}\\nu_1 + \\cdots + a_{kj}\\nu_k - \\lambda_j + c_j).$$\n",
    "\n",
    "Thus, as long as $a_{1j}\\nu_1 + \\cdots + a_{kj}\\nu_k - \\lambda_j + c_j = 0$ for all $1 \\leq j \\leq n$, we have\n",
    "\n",
    "$$g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = \\inf_{\\boldsymbol{x} \\in \\mathbb{R}^n}L(\\boldsymbol{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = -b_1\\nu_1 - \\cdots - b_n\\nu_n.$$\n",
    "\n",
    "On the other hand, if $a_{1j}\\nu_1 + \\cdots + a_{kj}\\nu_k - \\lambda_j + c_j \\neq 0$ for some $1 \\leq j \\leq n$, we have\n",
    "\n",
    "$$g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = \\inf_{\\boldsymbol{x} \\in \\mathbb{R}^n}L(\\boldsymbol{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = - \\infty,$$\n",
    "\n",
    "as we can take $x_j \\in \\mathbb{R}$ as small or large as possible. Thus, the Lagrange dual problem in this case is equivalent to the following.\n",
    "\n",
    "* Maximimize $-b_1\\nu_1 - \\cdots -b_k\\nu_k$ subject to\n",
    "* $A^T\\boldsymbol{\\nu} + \\boldsymbol{c} \\geq \\boldsymbol{\\lambda}$ and\n",
    "* $\\boldsymbol{\\lambda} \\geq \\boldsymbol{0}$,\n",
    "\n",
    "where $\\boldsymbol{c} = (c_1, \\dots, c_n)$. The two conditions can be shortened as follows:\n",
    "\n",
    "* maximimize $-b_1\\nu_1 - \\cdots -b_k\\nu_k$ subject to\n",
    "* $A^T\\boldsymbol{\\nu} + \\boldsymbol{c} \\geq \\boldsymbol{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong duality and Slater's condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, the weak duality lemma tells us that for any optimal choice $\\boldsymbol{x}^* \\in \\mathbb{R}^n$, which necessarily satsifies $\\boldsymbol{x}^* \\geq 0$, the bound from the above lemma gives us\n",
    "$$\\boldsymbol{c}^T\\boldsymbol{x}^* + \\boldsymbol{b}\\boldsymbol{\\nu} \\geq 0$$\n",
    "\n",
    "as long as $A^T\\boldsymbol{\\nu} + \\boldsymbol{c} \\geq \\boldsymbol{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's surprising is that it turns out that with the condition as long as $A^T\\boldsymbol{\\nu} + \\boldsymbol{c} \\geq \\boldsymbol{0}$, we neceesarily get the equality\n",
    "\n",
    "$$\\boldsymbol{c}^T\\boldsymbol{x}^* + \\boldsymbol{b}\\boldsymbol{\\nu} = 0.$$\n",
    "\n",
    "In other words, our optimal solution $\\boldsymbol{x}^* = (x_1, \\dots, x_n)$ turns out to satisfy an extra equality constraint\n",
    "$$c_1x_1 + \\cdots + c_nx_n = -b_1\\nu_1 - \\cdots - b_k\\nu_k.$$\n",
    "\n",
    "In our general set-up, this is when the optimal duality gap is $0$, and when this happens, we say that our optimization problem admits a **strong duality**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall prove the strong duality of the previous example in a more general setting. Before that, we define the [**relative interior**](https://en.wikipedia.org/wiki/Relative_interior#cite_note-3) $\\mathrm{relint}(S)$ of a subset $S \\subset \\mathbb{R}^n$ as the interior of $S$ with respect to the subspace topology in the **affine hull** of $S$, which is defined as the smallest affine subspace of $\\mathbb{R}^n$ containing $S$. More explicitly, we have\n",
    "\n",
    "$$\\mathrm{aff}(S) := \\{a_1s_1 + \\cdots + a_ls_l : l \\in \\mathbb{Z}_{\\geq 1} \\text{ and } a_1, \\dots, a_l \\in \\mathbb{R} \\text{ such that } a_1 + \\cdots + a_l = 1\\},$$\n",
    "\n",
    "but it is better to be thought as a space given by translating the origin of a sub-vector space of $\\mathbb{R}^n$ that is the smallest among such that contain $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem (Slater's condition)**. Consider the following optimization problem:\n",
    "* maximize $f_0(\\boldsymbol{x})$ subject to\n",
    "* $f_1(\\boldsymbol{x}), \\dots, f_m(\\boldsymbol{x}) \\leq 0$ and\n",
    "* $A \\boldsymbol{x} = \\boldsymbol{b}$\n",
    "\n",
    "for a fixed $k \\times n$ real matrix $A = (a_{ij})$ and $\\boldsymbol{b} \\in \\mathbb{R}^k$.\n",
    "\n",
    "Suppose that\n",
    "* $f_0, f_1, \\dots, f_m$ are [convex](https://en.wikipedia.org/wiki/Convex_function),\n",
    "* $f_1(\\tilde{\\boldsymbol{x}}), \\dots, f_m(\\tilde{\\boldsymbol{x}}) < 0$ and $A\\tilde{\\boldsymbol{x}} = \\boldsymbol{b}$ for some $\\tilde{\\boldsymbol{x}} \\in \\mathrm{relint}(D)$,\n",
    "* the optimization problem has a feasible solution $\\boldsymbol{x}^* \\in D$ such that $f_0(\\boldsymbol{x}^*)$ is not necessarily finite, and\n",
    "* there exists a dual optimal solution $(\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*) \\in \\mathbb{R}^m \\times \\mathbb{R}^k$ with $\\boldsymbol{\\lambda}^* \\geq \\boldsymbol{0}$ that is *feasible*, meaning that $g(\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*) > -\\infty$.\n",
    "\n",
    "Then <u>the optimization problem admits a strong duality</u>. In particualr, it follows that $f_0(\\boldsymbol{x}^*)$ is finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof*. Recall that our ongoing assumption throughout this notebook is that $D$ is nonempty. If the interior of $D$ in $\\mathbb{R}^n$ is nonempty, then the affine hull of $D$ is the whole space $\\mathbb{R}^n$, so $\\mathrm{relint}(D) = \\mathrm{int}(D)$. If $D$ has the empty interior in $\\mathbb{R}^n$, then we consider the affine hull of $D$, which is of the form $\\boldsymbol{w} + V$ for some $\\boldsymbol{w} \\in \\mathbb{R}^n$ and a sub-vector space $V$ of dimension $d$ with $0 \\leq d < n$. This lets us replace our $\\mathbb{R}^n$ with $\\mathbb{R}^d$ in our problem, so we can assume that $\\mathrm{relint}(D) = \\mathrm{int}(D)$ for the rest of the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we note that we can replace $A$ with its [reduced row echlon form](https://en.wikipedia.org/wiki/Row_echelon_form) and eliminate any zero rows to reduce our problem to the case where the rank of $A$ is exactly $k$. We also assumed that there exists a feasible dual optimal solution $(\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*) \\in \\mathbb{R}^m \\times \\mathbb{R}^k$ with $\\boldsymbol{\\lambda}^* \\geq \\boldsymbol{0}$. By the weak duality lemma, this implies that $f_0(\\boldsymbol{x}^*)$ is either finite or $\\infty$, but not $- \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following set is a [convex subset](https://en.wikipedia.org/wiki/Convex_set) of $\\mathbb{R}^{1+m+k}$ because $f_0, f_1, \\dots, f_m$ are convex:\n",
    "\n",
    "$$\\mathcal{A} := \\left\\{\\begin{array}{c}\n",
    "(t, \\boldsymbol{u}, \\boldsymbol{v}) \\in \\mathbb{R} \\times \\mathbb{R}^m \\times \\mathbb{R}^k: \\\\ \\text{there exists }\\boldsymbol{x} \\in D \\text{ such that}\\\\\n",
    "f_0(\\boldsymbol{x}) \\leq t, \\\\\n",
    "f_j(\\boldsymbol{x}) \\leq u_j \\text{ for } 1 \\leq j \\leq m, \\text{ and} \\\\\n",
    "a_{i1}x_1 + \\cdots + a_{in}x_n = v_i \\text{ for } 1 \\leq i \\leq k\n",
    "\\end{array}\\right\\}.$$\n",
    "\n",
    "We also consider the convsex subset $\\mathcal{B} := \\{(s, \\boldsymbol{0}, \\boldsymbol{0}) \\in \\mathbb{R} \\times \\mathbb{R}^m \\times \\mathbb{R}^k : s < f_0(\\boldsymbol{x}^*)\\}$.\n",
    "\n",
    "Note that\n",
    "$$\\mathcal{A} = (f_0,f_1, \\dots, f_m, h_1, \\dots, h_k)(D) + (\\mathbb{R}_{\\geq 0} \\times \\mathbb{R}_{\\geq 0}^m \\times \\{0\\}^k),$$\n",
    "\n",
    "where $h_i(\\boldsymbol{x}) = h_i(x_1, \\dots, x_n) = a_{i1}x_1 + \\cdots + a_{in}x_n$. From the second expression, it is easy to see that $\\mathcal{A}$ is an infinte set. Since $f_0(\\boldsymbol{x}^*) > -\\infty$, we see that $\\mathcal{B}$ is nonempty.\n",
    "\n",
    "Here, we note that $\\mathcal{A} \\cap \\mathcal{B} = \\emptyset$ by the minimality of $f_0(\\boldsymbol{x}^*)$. Since the above two are nonempty convex subsets of a real vector space, we can apply the [Hyperplane Separation Theorem](https://en.wikipedia.org/wiki/Hyperplane_separation_theorem) to find a nonzero vector $(\\mu, \\tilde{\\boldsymbol{\\lambda}}, \\tilde{\\boldsymbol{\\nu}}) \\in \\mathbb{R} \\times \\mathbb{R}^m \\times \\mathbb{R}^k$ and $\\alpha \\in \\mathbb{R}$ such that\n",
    "\n",
    "1. $\\mathcal{A} \\subset \\{(t, \\boldsymbol{u}, \\boldsymbol{v}) \\in \\mathbb{R} \\times \\mathbb{R}^m \\times \\mathbb{R}^k : \\tilde{\\boldsymbol{\\lambda}}^T \\boldsymbol{u} + \\tilde{\\boldsymbol{\\nu}}^T \\boldsymbol{v} + \\mu t \\geq \\alpha\\}$ and\n",
    "2. $\\mathcal{B} \\subset \\{(t, \\boldsymbol{u}, \\boldsymbol{v}) \\in \\mathbb{R} \\times  \\mathbb{R}^m \\times \\mathbb{R}^k: \\tilde{\\boldsymbol{\\lambda}}^T \\boldsymbol{u} + \\tilde{\\boldsymbol{\\nu}}^T \\boldsymbol{v} + \\mu t \\leq \\alpha\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying $(t, \\boldsymbol{u}, \\boldsymbol{v})$ in $\\mathcal{A}$, the first condition\n",
    "$$\\tilde{\\boldsymbol{\\lambda}}^T \\boldsymbol{u} + \\tilde{\\boldsymbol{\\nu}}^T \\boldsymbol{v} + \\mu t \\geq \\alpha$$\n",
    "\n",
    "tells us that we must have $\\boldsymbol{\\nu} \\geq \\boldsymbol{0}$ and $\\mu \\geq 0$ as otherwise, we can find an infinite sequence of $(t, \\boldsymbol{u}, \\boldsymbol{v})$ in $\\mathcal{A}$ such that $\\tilde{\\boldsymbol{\\lambda}}^T \\boldsymbol{u} + \\mu t$ diverges to $-\\infty$, which would contradict the above condition.\n",
    "\n",
    "Furthermore, the second condition ensures that for any real number $t < f_0(\\boldsymbol{x}^*)$, we must have $\\mu t \\leq \\alpha$, which implies that $\\mu f_0(\\boldsymbol{x}^*) \\leq \\alpha$. \n",
    "\n",
    "For any $\\boldsymbol{x} \\in D$, if we take \n",
    "* $t = f_0(\\boldsymbol{x})$, \n",
    "* $\\boldsymbol{u} = (f_1(\\boldsymbol{x}), \\dots, f_m(\\boldsymbol{x}))$, and\n",
    "* $\\boldsymbol{v} = (h_1(\\boldsymbol{x}), \\dots, h_k(\\boldsymbol{x}))$,\n",
    "\n",
    "we have $(t, \\boldsymbol{u}, \\boldsymbol{v}) \\in \\mathcal{A}$. This implies that\n",
    "$$\\tilde{\\lambda}_1f_1(\\boldsymbol{x}) + \\cdots + \\tilde{\\lambda}_mf_m(\\boldsymbol{x}) + \\tilde{\\boldsymbol{\\nu}}^T (A\\boldsymbol{x} - \\boldsymbol{b}) + \\mu f_0(\\boldsymbol{x}) \\geq \\alpha \\geq \\mu f_0(\\boldsymbol{x}^*).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already deduced that $\\mu \\geq 0$, which puts the rest of the proof in the following two cases:\n",
    "\n",
    "**Case 1**. $\\mu > 0$.\n",
    "\n",
    "Considering $\\boldsymbol{\\lambda} := \\tilde{\\boldsymbol{\\lambda}}/\\mu \\geq 0$ and $\\boldsymbol{\\nu} := \\tilde{\\boldsymbol{\\nu}}/\\mu$, the previous inequality gives us\n",
    "$$L(\\boldsymbol{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) \\geq f_0(\\boldsymbol{x}^*)$$\n",
    "\n",
    "for any $\\boldsymbol{x} \\in D$. Taking the infimum over $\\boldsymbol{x} \\in D$ thus gives us\n",
    "\n",
    "$$g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) \\geq f_0(\\boldsymbol{x}^*),$$\n",
    "\n",
    "which is the opposite inequality of the duality lemma. Thus, we have\n",
    "\n",
    "$$g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = f_0(\\boldsymbol{x}^*),$$\n",
    "\n",
    "which finishes the proof for this case. (This case does not use the Slater's condition.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2** $\\mu = 0$.\n",
    "\n",
    "This implies that\n",
    "$$\\tilde{\\lambda}_1f_1(\\boldsymbol{x}) + \\cdots + \\tilde{\\lambda}_mf_m(\\boldsymbol{x}) + \\tilde{\\boldsymbol{\\nu}}^T (A\\boldsymbol{x} - \\boldsymbol{b}) + \\geq 0$$\n",
    "\n",
    "for any $\\boldsymbol{x} \\in D$. We now take $\\boldsymbol{x} = \\tilde{\\boldsymbol{x}}$, which gives us\n",
    "\n",
    "$$\\tilde{\\lambda}_1f_1(\\tilde{\\boldsymbol{x}}) + \\cdots + \\tilde{\\lambda}_mf_m(\\tilde{\\boldsymbol{x}}) \\geq 0.$$\n",
    "\n",
    "Since $f_1(\\tilde{\\boldsymbol{x}}), \\dots, f_m(\\tilde{\\boldsymbol{x}}) < 0$, the above inequality implies that $\\tilde{\\boldsymbol{\\lambda}} = \\boldsymbol{0}$. Since $\\mu = 0$ and $(\\mu, \\tilde{\\boldsymbol{\\lambda}}, \\tilde{\\boldsymbol{\\nu}}) \\neq \\boldsymbol{0}$, we must have that $\\tilde{\\boldsymbol{\\nu}} \\neq \\boldsymbol{0}$ in $\\mathbb{R}^k$. Now, reading the inequality before Case 1 gives us \n",
    "\n",
    "$$\\tilde{\\boldsymbol{\\nu}}^T (A\\boldsymbol{x} - \\boldsymbol{b}) \\geq \\boldsymbol{0}$$\n",
    "\n",
    "for all $\\boldsymbol{x} \\in D$. Since the rank of $A$ is $k$ and $\\tilde{\\boldsymbol{\\nu}} \\neq \\boldsymbol{0}$, we must have $A^T \\tilde{\\boldsymbol{\\nu}} \\neq \\boldsymbol{0}$ in $\\mathbb{R}^n$. We thus get the following nontrivial affine hyperplane:\n",
    "\n",
    "$$H := \\{\\boldsymbol{x} \\in \\mathbb{R}^n : \\tilde{\\boldsymbol{\\nu}}^T(A\\boldsymbol{x} - \\boldsymbol{b}) = \\boldsymbol{0}\\}.$$\n",
    "\n",
    "Since $A\\tilde{\\boldsymbol{x}} = \\boldsymbol{b}$, we know $\\boldsymbol{x}$ lies in the hyperplane. Moreover, since $\\boldsymbol{x} \\in \\mathrm{int}(D)$, there are infinitely many points $\\boldsymbol{x} \\in D$ such that \n",
    "$$\\tilde{\\boldsymbol{\\nu}}^T(A\\boldsymbol{x} - \\boldsymbol{b}) < \\boldsymbol{0},$$\n",
    "\n",
    "which contradicts our previous deduction. Thus, Case 2 never happens. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**. It is worth noting that the proof also provides an algorithm to constuct $(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$ such that $$g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = f_0(\\boldsymbol{x}^*).$$\n",
    "\n",
    "Moreover, the proof also reveals that if an optimal solution $\\boldsymbol{x}^* \\in D$ exists such that $f_0(\\boldsymbol{x}^*)$ is finite, then a dual optimal solution exists and gives a strong duality (using the algorithm described by above)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
